{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgSDrXY_5tv"
      },
      "source": [
        "# FIT5226 Assignment 2 - Deep Q Learning\n",
        "\n",
        "Team Name: Simple <br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Team Members: Satoshi Kashima, Shosuke Asano, Tanul Gupta, Felix Tay Shi Hong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptQ36NY__5nM"
      },
      "source": [
        "**<p>Action:</p>**\n",
        "\n",
        "We have 5 actions: going left, right, down, up, and collecting an item. The action other than collecting items is generally available to all states (except where itâ€™s inappropriate e.g. at the right-top corner, only available actions should be left and down actions). The action to collect the item is only available at the first time visiting the item location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TkAgg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fZ5gJ4MKghh"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Action(Enum):\n",
        "    # NOTE: QValue matrix used these action values as their indices\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "    DOWN = 2\n",
        "    UP = 3\n",
        "\n",
        "    # actions when agent just got the item and is moving to item_reached state\n",
        "    COLLECT = 4  # goes to item reached state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmfE4waBXyQ"
      },
      "source": [
        "**<p>Environment Setting</p>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrnN1JzgMVZI"
      },
      "source": [
        "Assignment2State is an extension of the State class, adding attributes like goal_location, goal_direction, and item_direction. These additional attributes provide more context about the agent's environment, such as the direction to the goal and item. This increases our statespace size to 11 inputs to our DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m5aRXEOzKmpv"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class State:\n",
        "    # it doesn not hold AgentObject / ItemObject because I want State to be immutable\n",
        "    # but in the future, we might want to add more attributes to State\n",
        "    # in that case we need to make a copy of the AgentObject / ItemObject\n",
        "    agent_location: tuple[int, int]\n",
        "    item_location: tuple[int, int]\n",
        "    has_item: bool = False\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class Assignment2State(State):\n",
        "    goal_location: tuple[int, int]\n",
        "\n",
        "    # https://edstem.org/au/courses/17085/discussion/2192014\n",
        "    # these two attributes should be vectors (does not need to be unit vectors as we use cos distance)\n",
        "    # but these should be in terms of coordinates, not indices so be careful\n",
        "    goal_direction: tuple[float, float]\n",
        "    item_direction: tuple[float, float]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLMqLBCnjwOp"
      },
      "source": [
        "**<P>Environment Setup</p>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPI0C-j0jwOp"
      },
      "source": [
        "<p>The Assignment2Environment class is a wrapper for handling multiple sub-environments. Each sub-environment has different goal and item locations, providing more varied scenarios for training the agent. The class manages the initialization of these environments and the state transitions between them, utilizing the extended state information provided by Assignment2State.\n",
        "This class introduces direction-based rewards using cosine similarity, enhancing the agent's ability to learn by incorporating directional cues towards goals and items.</p>\n",
        "\n",
        "<p>The Environment class itself has been enhanced to support more flexible initializations and interactions. It includes additional parameters for penalties and rewards and improved handling of animations for visualizing agent movements and decisions.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYA0AB8jwOq"
      },
      "source": [
        "**<p>Visualization</p>**\n",
        "\n",
        "<p>The visualization in the environment is set up to represent the agent's behavior within an n x n grid world. The agent (A) moves around the grid to pick up an item (I) and reach a goal (G), while the entire process is animated using Matplotlib. The visualizations are designed to provide intuitive feedback about the agent's decision-making process and its learning progress.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lnhIAP2jwOq"
      },
      "source": [
        "**<p>Initialization:</p>**\n",
        "\n",
        "<ol>\n",
        "<li>The environment is initialized with parameters n (representing the size of the grid n x n) and with_animation (a boolean indicating whether to show animations or not).</li>\n",
        "<li>During the initialization, the initialize_for_new_episode function is called to create the plot using Matplotlib, and the animate function is called to set up the elements inside the plot.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JASnWqAtjwOq"
      },
      "source": [
        "**<p>animate() Function:</p>**\n",
        "Setup: Sets up a grid of size n x n and displays it using Matplotlib.  \n",
        "Icons:\n",
        "<ol>\n",
        "<li>A (Agent): Represents the \"person\" or the agent navigating through the grid.</li>\n",
        "<li>I (Item): Represents the \"item\" that the agent needs to collect.</li>\n",
        "<li>G (Goal): Represents the \"goal\" or the final destination that the agent needs to reach after collecting the item.</li>\n",
        "<li>Legend: A legend is displayed to help identify the icons (A, I, G) on the grid.</li>\n",
        "</ol>\n",
        "\n",
        "**<p>Positions:</p>**\n",
        "*   The goal (G) is now positioned randomly in the grid.\n",
        "\n",
        "**<p>Agent State Change:</p>**\n",
        "*   When the agent picks up the item (I), its color changes, indicating that it is now carrying the item. The legend is updated to reflect this change.\n",
        "\n",
        "**<p>Visual Updates:</p>**\n",
        "*   A small delay (0.7 seconds) is added to allow visualization of each movement or state change on the grid, providing a smooth animation experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAcrhKDZjwOq"
      },
      "source": [
        "**<p>step() Function</p>**\n",
        "\n",
        "*   This function is responsible for causing the movement of the agent within the grid.\n",
        "*   The agent selects an action (such as moving left, right, up, down, or collecting the item)\n",
        "*   The action is performed if it is valid (e.g., staying within grid boundaries), and the agent's state is updated accordingly.\n",
        "*   After the action is performed, the animate function is called again to display the new state of the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM_RFtk-m98Z"
      },
      "source": [
        "**<p>Reward Structure</p>**\n",
        "\n",
        "For the reward structure, please see the `get_reward` function of the `Environment` class. We give an item collection reward of `self.item_state_reward` when the agent collects the item for the first time by visiting item location correctly. `self.item_revisit_penalty` gives a penalty for revisiting the item position. We assign a large goal reward of `self.goal_state_reward` for reaching the goal state with the item, and we apply a penalty of `self.goal_no_item_penalty` if the agent reaches the goal without the item. The goal state is only valid when the item has been collected; reaching the goal before collecting the item results in a large penalty to discourage this. Apart from these cases, a time penalty of `self.time_penalty` is consistently applied to encourage efficiency in completing the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SHTjt8ydKo2o"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from abc import ABC\n",
        "from random import randint, choice\n",
        "from typing import cast\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "DEFAULT_TIME_PENALTY = -1\n",
        "GOAL_STATE_REWARD = 200\n",
        "DEFAULT_ITEM_REWARD = 300\n",
        "DEFAULT_ITEM_REVISIT_PENALTY = -200\n",
        "DEFAULT_GOAL_NO_ITEM_PENALTY = -300\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: int = 5,\n",
        "        item: ItemObject | None = None,\n",
        "        goal_location: tuple[int, int] = (4, 0),\n",
        "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
        "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
        "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
        "        item_revisit_penalty: int | float = DEFAULT_ITEM_REVISIT_PENALTY,\n",
        "        goal_no_item_penalty: int | float = DEFAULT_GOAL_NO_ITEM_PENALTY,\n",
        "        with_animation: bool = True,\n",
        "    ) -> None:\n",
        "        self.n = n\n",
        "        self.goal_location = goal_location\n",
        "        self.time_penalty = time_penalty\n",
        "        self.item_state_reward = item_state_reward\n",
        "        self.goal_state_reward = goal_state_reward\n",
        "        self.item_revisit_penalty = item_revisit_penalty\n",
        "        self.goal_no_item_penalty = goal_no_item_penalty\n",
        "\n",
        "        self.item = ItemObject() if item is None else item\n",
        "        self.agent = AgentObject()\n",
        "\n",
        "        if self.item.location is None:\n",
        "            self.item.set_location_randomly(self.n, self.n)\n",
        "\n",
        "        self.state: State\n",
        "        # TODO: possibly implmeent this if there are multiple GridObjects to check for\n",
        "        # initialize grid and put grid objects on the grid\n",
        "        # x_agent, y_agent = self.agent.location\n",
        "        # x_item, y_item = self.item.location\n",
        "        # self.grid = np.zeros((self.n, self.n))\n",
        "        # self.grid[x_agent, y_agent] = self.agent\n",
        "        # self.grid[x_item, y_item] = self.item\n",
        "\n",
        "        # Setup for animation\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None) -> None:\n",
        "        if agent_location is None:\n",
        "            self.agent.set_location_randomly(self.n, self.n,)\n",
        "        else:\n",
        "            self.agent.location = agent_location\n",
        "        self.agent.has_item = False if randint(0, 1) == 0 else True\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "\n",
        "        # ensure that no multiple matplotlib windows open\n",
        "        if hasattr(self, \"fig\"):\n",
        "            plt.close(self.fig)  # type: ignore\n",
        "        self.fig, self.ax = plt.subplots(figsize=(8, 8)) if self.with_animation else (None, None)\n",
        "        self.animate()  # Initial drawing of the grid\n",
        "\n",
        "        # Reset the last action and reward\n",
        "        self.last_action = None\n",
        "        self.last_reward = None\n",
        "\n",
        "\n",
        "    def get_state(self) -> State:\n",
        "        return self.state\n",
        "\n",
        "    def set_with_animation(self, with_animation: bool) -> None:\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def get_available_actions(self, state: State | None = None) -> list[Action]:\n",
        "        \"\"\"\n",
        "        Assumes that the current state is not the goal state\n",
        "        \"\"\"\n",
        "        # logic to determine available actions\n",
        "        actions = []\n",
        "        current_state = state if state is not None else self.state\n",
        "        x, y = current_state.agent_location\n",
        "\n",
        "        if current_state.agent_location == current_state.item_location and not current_state.has_item:\n",
        "            actions.append(Action.COLLECT)\n",
        "\n",
        "        # note: technically speaking we know that whenever agent is at the item location, the only available (or, the most optimal) action is to collect the item\n",
        "        # however, according to the CE, we must ensure that\n",
        "        # \"the agent is supposed to learn (rather than being told) that\n",
        "        # once it has picked up the load it needs to move to the delivery point to complete its mission. \",\n",
        "        # implyging that agent must be able to learn to \"collect\" instead of being told to collect (so add all possible actions)\n",
        "        if x > 0:\n",
        "            actions.append(Action.LEFT)  # left\n",
        "        if x < self.n - 1:\n",
        "            actions.append(Action.RIGHT)  # right\n",
        "        if y > 0:\n",
        "            actions.append(Action.DOWN)  # down\n",
        "        if y < self.n - 1:\n",
        "            actions.append(Action.UP)  # up\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def get_reward(self, prev_state: State, current_state: State, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the reward based on the agent's actions and state transitions.\n",
        "        \"\"\"\n",
        "        reward = self.time_penalty\n",
        "\n",
        "        # Large penalty for reaching the goal without the item\n",
        "        if current_state.agent_location == self.goal_location and not current_state.has_item:\n",
        "            reward += self.goal_no_item_penalty\n",
        "            return reward\n",
        "\n",
        "        # Large reward for reaching the goal with the item\n",
        "        if self.is_goal_state(current_state):\n",
        "            reward += self.goal_state_reward\n",
        "\n",
        "        # Reward for collecting the item\n",
        "        if not prev_state.has_item and current_state.agent_location == current_state.item_location:\n",
        "            reward += self.item_state_reward\n",
        "        if action == Action.COLLECT and prev_state.agent_location == current_state.item_location and not prev_state.has_item:\n",
        "            reward += self.item_state_reward\n",
        "\n",
        "        # Penalty for revisiting item location\n",
        "        if action == Action.COLLECT and (prev_state.has_item or prev_state.agent_location != current_state.item_location):\n",
        "            reward += self.item_revisit_penalty\n",
        "        if prev_state.has_item and current_state.agent_location == current_state.item_location:\n",
        "            reward += self.item_revisit_penalty\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def update_state(self, action: Action) -> None:\n",
        "        \"\"\"\n",
        "        Be careful: this method updates the state of the environment\n",
        "        \"\"\"\n",
        "        self.agent.move(action,self.n)\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "\n",
        "    def is_goal_state(self, state: State) -> bool:\n",
        "        return self.state.has_item and self.goal_location == state.agent_location\n",
        "\n",
        "    def animate(self, state: Assignment2State | None = None, prev_state: Assignment2State | None = None, is_greedy: bool | None = None, all_qvals: np.ndarray | None = None, chosen_action: Action | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Animates the action\n",
        "        (basically just prints out the new state, but because it seems like the agent is \"moving\" because it's updated in the same figure)\n",
        "        \"\"\"\n",
        "        if not self.with_animation:\n",
        "            return\n",
        "        self.ax.clear()\n",
        "        self.ax.set_xlim(0, self.n)\n",
        "        self.ax.set_ylim(0, self.n)\n",
        "        self.ax.set_xticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.set_yticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.grid(True)\n",
        "\n",
        "        # Plotting the agent, item, and goal\n",
        "        self.ax.text(\n",
        "            self.agent.location[0] + 0.5,\n",
        "            self.agent.location[1] + 0.5,\n",
        "            \"A\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"blue\" if not self.agent.has_item else \"purple\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.item.location[0] + 0.5,\n",
        "            self.item.location[1] + 0.5,\n",
        "            \"I\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"green\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.goal_location[0] + 0.5,\n",
        "            self.goal_location[1] + 0.5,\n",
        "            \"G\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"red\",\n",
        "        )\n",
        "\n",
        "        # FIXME: this doesn't work for State (only works for Assignment2State)\n",
        "        state_str = str(self.state) if state is None else str(state)\n",
        "        state_text = \"\".join(state_str.split(',')[:5]) + '\\n' + \"\".join(state_str.split(',')[5:])\n",
        "\n",
        "        # show state info\n",
        "        self.ax.text(\n",
        "            2,\n",
        "            2,\n",
        "            state_text,\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=10,\n",
        "            color=\"orange\",\n",
        "        )\n",
        "\n",
        "        # prints: if the action selected was greedy or random\n",
        "        if is_greedy is not None:\n",
        "            self.ax.text(\n",
        "            self.n,\n",
        "            self.n,\n",
        "            \"Action is greedy\" if is_greedy else \"Action is random\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=10,\n",
        "            color=\"black\",\n",
        "        )\n",
        "\n",
        "        # prints the q values for all possible actions in the previous state\n",
        "        # note: this is only printed if the action was greedy (because if random, the q values did not matter for action selection)\n",
        "        # note2: only \"possible\" actions are printed i.e. (if agent is not at the item position, it does not print the collect q value)\n",
        "        if all_qvals is not None and prev_state is not None and is_greedy:\n",
        "            left_q, right_q, down_q, up_q, collect_q = all_qvals\n",
        "            possible_actions = self.get_available_actions(prev_state)\n",
        "            # show left q value\n",
        "            prev_agent_location_on_plot_x = prev_state.agent_location[0] + 0.5\n",
        "            prev_agent_location_on_plot_y = prev_state.agent_location[1] + 0.5\n",
        "            box_center_to_val_location = 0.3\n",
        "            if Action.LEFT in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x - box_center_to_val_location,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{left_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.LEFT else \"black\",\n",
        "                )\n",
        "            if Action.RIGHT in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x + box_center_to_val_location,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{right_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.RIGHT else \"black\",\n",
        "                )\n",
        "            if Action.DOWN in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y - box_center_to_val_location,\n",
        "                    f'{down_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.DOWN else \"black\",\n",
        "                )\n",
        "            if Action.UP in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y + box_center_to_val_location,\n",
        "                    f'{up_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.UP else \"black\",\n",
        "                )\n",
        "            if Action.COLLECT in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{collect_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.COLLECT else \"black\",\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: add a message saying \"item collected\" if the agent has collected the item\n",
        "        # or else there is a single frame where the agent is at the same location twice,\n",
        "        # so it looks like the agent is not moving\n",
        "        handles = [\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"blue\", markersize=8, label=\"Agent (A)\")\n",
        "            if not self.agent.has_item\n",
        "            else plt.Line2D(\n",
        "                [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"purple\", markersize=8, label=\"Agent (A) with item\"\n",
        "            ),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"green\", markersize=8, label=\"Item (I)\"),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"red\", markersize=8, label=\"Goal (G)\"),\n",
        "        ]\n",
        "        self.ax.legend(handles=handles, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "        plt.subplots_adjust(right=0.75, left=0.1)\n",
        "        self.fig.canvas.draw_idle()\n",
        "        plt.pause(0.7)  # Pause to allow visualization of the movement\n",
        "\n",
        "    def step(self, action: Action) -> tuple[float, State]:\n",
        "        prev_state = self.get_state()\n",
        "        self.update_state(action)\n",
        "        next_state = self.get_state()\n",
        "        self.animate()\n",
        "        reward = self.get_reward(prev_state, next_state,action)\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "class Assignment2Environment:\n",
        "    \"\"\"\n",
        "    A wrapper class for multiple environments for Assignment 2\n",
        "    This environment consits of multiple \"sub-environments\" where each sub-environment has a different goal and item location\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: int = 5,\n",
        "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
        "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
        "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
        "        direction_reward_multiplier: int | float = 10,\n",
        "        with_animation: bool = True,\n",
        "    ) -> None:\n",
        "        self.n = n\n",
        "        # initialize a list of environments for all possible goal and item positions\n",
        "        self.environments = []\n",
        "\n",
        "        for goal_x in range(self.n):\n",
        "            for goal_y in range(self.n):\n",
        "                for item_x in range(self.n):\n",
        "                    for item_y in range(self.n):\n",
        "                        if (goal_x, goal_y) == (item_x, item_y):\n",
        "                            continue\n",
        "                        environment = Environment(\n",
        "                            n=self.n,\n",
        "                            goal_location=(goal_x, goal_y),\n",
        "                            item=ItemObject(location=(item_x, item_y)),\n",
        "                            with_animation=with_animation,\n",
        "                            time_penalty=time_penalty,\n",
        "                            item_state_reward=item_state_reward,\n",
        "                            goal_state_reward=goal_state_reward,\n",
        "                        )\n",
        "                        self.environments.append(environment)\n",
        "\n",
        "        self.environments = [self.environments[10]]\n",
        "\n",
        "\n",
        "        self.direction_reward_multiplier = direction_reward_multiplier\n",
        "\n",
        "        self.current_sub_environment: Environment\n",
        "        self.state: Assignment2State\n",
        "\n",
        "    def get_random_sub_environment(self) -> Environment:\n",
        "        return choice(self.environments)\n",
        "\n",
        "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None, index: int | None = None) -> None:\n",
        "        self.current_sub_environment = self.get_random_sub_environment() if index is None else self.environments[index]\n",
        "        self.current_sub_environment.initialize_for_new_episode(agent_location)\n",
        "\n",
        "        self.state = Assignment2State(\n",
        "            agent_location=self.current_sub_environment.agent.get_location(),\n",
        "            item_location=self.current_sub_environment.item.get_location(),\n",
        "            has_item=self.current_sub_environment.agent.has_item,\n",
        "            goal_location=self.current_sub_environment.goal_location,\n",
        "            goal_direction=self.get_goal_direction(),\n",
        "            item_direction=self.get_item_direction(),\n",
        "        )\n",
        "        # NOTE: animation should be handled by individual sub-environments\n",
        "\n",
        "    def get_available_actions(self, state:Assignment2State) -> list[Action]:\n",
        "        return self.current_sub_environment.get_available_actions(state)\n",
        "\n",
        "    def set_with_animation(self, with_animation: bool) -> None:\n",
        "        for environment in self.environments:\n",
        "            environment.set_with_animation(with_animation)\n",
        "\n",
        "    def get_direction_reward(self, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        Use cosine similarity to calculate the reward based on the direction of the action.\n",
        "        \"\"\"\n",
        "        has_collected_item = self.state.has_item\n",
        "\n",
        "        # Define action direction vectors\n",
        "        if action == Action.LEFT:\n",
        "            action_direction = (-1, 0)\n",
        "        elif action == Action.RIGHT:\n",
        "            action_direction = (1, 0)\n",
        "        elif action == Action.DOWN:\n",
        "            action_direction = (0, -1)\n",
        "        elif action == Action.UP:\n",
        "            action_direction = (0, 1)\n",
        "        else:\n",
        "            action_direction = (0, 0)  # Invalid action, handle accordingly\n",
        "\n",
        "        # Calculate the direction reward based on the goal or item direction\n",
        "        if has_collected_item:\n",
        "            target_direction = self.state.goal_direction\n",
        "        else:\n",
        "            target_direction = self.state.item_direction\n",
        "\n",
        "        # Check if either vector is zero to avoid division by zero\n",
        "        if np.linalg.norm(action_direction) == 0 or np.linalg.norm(target_direction) == 0:\n",
        "            return 0.0  # No direction reward if either vector is zero\n",
        "\n",
        "        # Calculate the cosine similarity (1 - cosine distance)\n",
        "        try:\n",
        "            reward = 1 - cosine(action_direction, target_direction)\n",
        "        except ValueError:\n",
        "            # Handle any errors from invalid vectors\n",
        "            reward = 0.0\n",
        "\n",
        "        return reward * self.direction_reward_multiplier\n",
        "\n",
        "\n",
        "    # def get_reward(self, prev_state: Assignment2State, current_state: Assignment2State, action: Action) -> float:\n",
        "    #     state_raward = self.current_sub_environment.get_reward(prev_state, current_state,action)\n",
        "    #     action_reward = self.get_direction_reward(action)\n",
        "    #     return state_raward + action_reward\n",
        "    def get_reward(self, prev_state: Assignment2State, current_state: Assignment2State, action: Action) -> float:\n",
        "        state_raward = self.current_sub_environment.get_reward(prev_state, current_state, action)\n",
        "        return state_raward\n",
        "\n",
        "\n",
        "    def get_state(self) -> Assignment2State:\n",
        "        return self.state\n",
        "\n",
        "    def get_goal_direction(self) -> tuple[float, float]:\n",
        "        return (\n",
        "            self.current_sub_environment.goal_location[0] - self.current_sub_environment.agent.get_location()[0],\n",
        "            self.current_sub_environment.goal_location[1] - self.current_sub_environment.agent.get_location()[1],\n",
        "        )\n",
        "\n",
        "    def get_item_direction(self) -> tuple[float, float]:\n",
        "        return (\n",
        "            self.current_sub_environment.item.get_location()[0] - self.current_sub_environment.agent.get_location()[0],\n",
        "            self.current_sub_environment.item.get_location()[1] - self.current_sub_environment.agent.get_location()[1],\n",
        "        )\n",
        "\n",
        "    def is_goal_state(self, state: State) -> bool:\n",
        "        return self.current_sub_environment.state.has_item and self.current_sub_environment.goal_location == state.agent_location\n",
        "\n",
        "    def update_state(self, action: Action) -> None:\n",
        "        \"\"\"\n",
        "        Be careful: this method updates the state of the environment\n",
        "        \"\"\"\n",
        "        self.current_sub_environment.update_state(action)\n",
        "        self.state = Assignment2State(\n",
        "            agent_location=self.current_sub_environment.agent.get_location(),\n",
        "            item_location=self.current_sub_environment.item.get_location(),\n",
        "            has_item=self.current_sub_environment.agent.has_item,\n",
        "            goal_location=self.current_sub_environment.goal_location,\n",
        "            goal_direction=self.get_goal_direction(),\n",
        "            item_direction=self.get_item_direction(),\n",
        "        )\n",
        "\n",
        "    def step(self, action: Action, is_greedy: bool, all_qvals: np.ndarray) -> tuple[float, Assignment2State]:\n",
        "        prev_state = self.get_state()\n",
        "        self.update_state(action)\n",
        "        next_state = self.get_state()\n",
        "        self.current_sub_environment.animate(self.get_state(), prev_state, is_greedy, all_qvals, action)\n",
        "        reward = self.get_reward(prev_state, next_state, action)\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "class GridObject(ABC):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        self.icon: str\n",
        "        self.location = (\n",
        "            location  # NOTE: location is a tuple of (x, y) where x and y are coordinates on the grid (not indices)\n",
        "        )\n",
        "\n",
        "    def set_location_randomly(\n",
        "        self, max_x: int, max_y: int, disallowed_locations: list[tuple[int, int]] = []\n",
        "    ) -> tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Note: max_x and max_y are exclusive\n",
        "\n",
        "        disallowed_locations: list of locations that are not allowed to be placed\n",
        "        (e.g. agent and item location should not be initialized to the same place)\n",
        "        \"\"\"\n",
        "        # The start, item, goal location must be different position\n",
        "        location = None\n",
        "        while location is None or location in disallowed_locations:\n",
        "            location = (randint(0, max_x - 1), randint(0, max_y - 1))\n",
        "\n",
        "        self.location = location\n",
        "        return location\n",
        "\n",
        "    def get_location(self) -> tuple[int, int]:\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Location is not set\")\n",
        "        return self.location\n",
        "\n",
        "\n",
        "class AgentObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        super().__init__(location)\n",
        "        self.icon = \"A\"\n",
        "        self.has_item = False  # TODO: has_item of AgentObject and State must be synched somehow\n",
        "\n",
        "    def move(self, action: Action, grid_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Move the agent based on the given action while ensuring it doesn't leave the bounds of the grid.\n",
        "        \"\"\"\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Agent location is not set\")\n",
        "\n",
        "        x, y = self.location\n",
        "\n",
        "        # Check each action and ensure it stays within the bounds\n",
        "        if action == Action.LEFT:\n",
        "            if x > 0:  # Ensure not moving out of bounds on the left\n",
        "                self.location = (x - 1, y)\n",
        "        elif action == Action.RIGHT:\n",
        "            if x < grid_size - 1:  # Ensure not moving out of bounds on the right\n",
        "                self.location = (x + 1, y)\n",
        "        elif action == Action.DOWN:\n",
        "            if y > 0:  # Ensure not moving out of bounds downwards\n",
        "                self.location = (x, y - 1)\n",
        "        elif action == Action.UP:\n",
        "            if y < grid_size - 1:  # Ensure not moving out of bounds upwards\n",
        "                self.location = (x, y + 1)\n",
        "        elif action == Action.COLLECT:\n",
        "            self.has_item = True  # Action to collect the item (no bounds check needed)\n",
        "\n",
        "\n",
        "\n",
        "class ItemObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None):\n",
        "        super().__init__(location)\n",
        "        self.icon = \"I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrtXxxo2Tq2Y"
      },
      "source": [
        "**<p>Replay Buffer</p>**\n",
        "\n",
        "<p>BaseReplayBuffer, a base replay buffer defined to provide a common interface and basic functionality such as a deque that stores experiences and the maximum number of experiences that a buffer can hold. The remember function will add new experiences to the buffer and remove the oldest experience once the buffer reached its maximum size. The abstract method is intended to be implemented by subclasses to sample a batch of experiences from the buffer. It provides a way to retrieve multiple experiences at once for training the DQN agent.</p>\n",
        "\n",
        "<p> The object function ReplayBuffer aims to sample a random subset of experiences from the buffer without replacement and unpacks the sampled experiences into separate lists for states, actions, rewards, next states and done flags using zip(*batch).</p>\n",
        "\n",
        "<p>The object function PrioritizedExperienceBuffer extends BaseReplayBuffer to implement prioritized experience replay. This technique assigns different priorities to experiences based on their importance for the agent to focus on more significant experiences more frequently. We use an alpha value hyperparameter to control the prioritzation of experiences. It affects the degree to which priorities influence sampling probabilities.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tEu0uvTkEDYx"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Experience = tuple[np.ndarray, int, float, np.ndarray, bool]\n",
        "\n",
        "\n",
        "class BaseReplayBuffer(ABC):\n",
        "    def __init__(self, max_size: int):\n",
        "        self.max_size = max_size\n",
        "        self.buffer: deque[Experience] = deque([])\n",
        "\n",
        "    def remember(self, experience: Experience) -> None:\n",
        "        if len(self.buffer) >= self.max_size:\n",
        "            self.buffer.popleft()  # Remove the oldest experience\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    @abstractmethod\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ReplayBuffer(BaseReplayBuffer):\n",
        "    def __init__(self, max_size: int):\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences from the replay memory.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
        "        return (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            done_batch,\n",
        "        )\n",
        "\n",
        "\n",
        "class PrioritizedExperienceBuffer(BaseReplayBuffer):\n",
        "    def __init__(self, max_size: int, alpha: float = 0.6):\n",
        "        \"\"\"\n",
        "        alpha for prioritization\n",
        "        \"\"\"\n",
        "        super().__init__(max_size)\n",
        "        self.alpha = alpha\n",
        "        self.priorities = np.ones(max_size, dtype=np.float32)\n",
        "\n",
        "        self.sampled_indices: np.ndarray[int] | None = None\n",
        "\n",
        "    def remember(self, experience: Experience) -> None:\n",
        "        if len(self.buffer) >= self.max_size:\n",
        "            self.buffer.popleft()\n",
        "            self.priorities[:-1] = self.priorities[1:]\n",
        "\n",
        "        self.buffer.append(experience)\n",
        "        max_priority = self.priorities.max() if self.buffer else 1.0  # max to ensure newly added experience has the highest priority\n",
        "        self.priorities[len(self.buffer) - 1] = max_priority\n",
        "\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences from the replay memory.\n",
        "        \"\"\"\n",
        "        priorities = self.priorities[: len(self.buffer)]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        self.sampled_indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        samples = [self.buffer[idx] for idx in self.sampled_indices]\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*samples)\n",
        "        return (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            done_batch,\n",
        "        )\n",
        "\n",
        "    def update_priorities(self, priorities: np.ndarray) -> None:\n",
        "        if self.sampled_indices is None:\n",
        "            raise ValueError(\"You need to sample a batch before updating priorities\")\n",
        "        for idx, priority in zip(self.sampled_indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "        self.sampled_indices = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvVZJdBE8kY"
      },
      "source": [
        "**<p>DQN Agent that utilise Deep Q Network</p>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**<p>Initialization (__init__):</p>**\n",
        "*   The constructor sets up hyperparameters for the DQN agent such as learning rate (alpha), discount factor (discount_rate), exploration parameters (epsilon, epsilon_decay, epsilon_min), replay memory settings, and batch size.\n",
        "*   It initializes a prediction model and a target model using neural networks, which are crucial for the DQN algorithm.\n",
        "*   The agent uses an optimizer (AdamW) for training the model and a learning rate scheduler to adjust the learning rate dynamically.\n",
        "*   A replay buffer is created to store past experiences, enabling experience replay to stabilize training.\n",
        "\n",
        "**<p>Neural Network Preparation (prepare_torch):</p>**\n",
        "*   Defines a neural network model using PyTorch for approximating the Q-value function. The model consists of input, hidden, and output layers with ReLU activations.\n",
        "\n",
        "**<p>Target Network Update (update_target_network):</p>**\n",
        "*   Updates the weights of the target network using a soft update mechanism (tau) to keep it slowly in sync with the prediction network. This improves stability during training.\n",
        "\n",
        "**<p>Action Selection (select_action):</p>**\n",
        "*   Implements an Îµ-greedy policy for action selection. The agent explores with a certain probability (epsilon) or exploits the best-known action based on the Q-values from the prediction model.\n",
        "\n",
        "**<p>Q-Value Calculation (get_qvals, get_maxQ, get_double_q):</p>**\n",
        "*   Computes Q-values for given states using both the prediction and target networks to determine the optimal actions and targets for training.\n",
        "\n",
        "**<p>Training (train_one_step, replay):</p>**\n",
        "*   train_one_step: Performs a single step of gradient descent on the prediction network to minimize the difference between predicted and target Q-values.\n",
        "*   replay: Samples a batch of experiences from the replay buffer and uses them to train the model, improving its decision-making policy.\n",
        "Model Saving and Loading (save_state, load_state):\n",
        "\n",
        "**<p>Model Saving and Loading (save_state, load_state):</p>**\n",
        "*   Allows saving and loading of the agentâ€™s state, including model weights, optimizer state, exploration parameters, replay buffer, and random seeds, enabling the agent to be resumed from a previous state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "unppTePzKo3a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        statespace_size: int = 11,\n",
        "        action_space_size: int = len(Action),\n",
        "        alpha: float = 0.0005,\n",
        "        discount_rate: float = 0.99,\n",
        "        epsilon: float = 1,\n",
        "        epsilon_decay: float = 0.99997,\n",
        "        epsilon_min: float = 0.007,\n",
        "        replay_memory_size: int = 10000,\n",
        "        batch_size: int = 128,\n",
        "        min_replay_memory_size: int = 1000,\n",
        "        tau: float = 0.05,\n",
        "        with_log: bool = False,\n",
        "        loss_log_interval: int = 100,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the DQN Agent\n",
        "        \"\"\"\n",
        "        self.alpha = alpha  # learning rate for optimizer\n",
        "        self.discount_rate = discount_rate\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # rate at which exploration rate decays\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.min_replay_memory_size = min_replay_memory_size\n",
        "        \n",
        "        self.replay_buffer = PrioritizedExperienceBuffer(max_size=replay_memory_size)\n",
        "        \n",
        "        # Initialize DQN models\n",
        "        self.model = self.prepare_torch(statespace_size)  # prediction model\n",
        "        self.target_model = deepcopy(self.model)  # target model\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.alpha, amsgrad=True)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.9)\n",
        "        self.loss_fn = torch.nn.MSELoss(reduction='none')\n",
        "        self.steps = 0\n",
        "        \n",
        "        self.tau = tau  # for soft update of target parameters\n",
        "\n",
        "        # Internal data tracking for plotting purposes\n",
        "        self.logged_data = {\n",
        "            'avg_predicted_qval': [],\n",
        "            'avg_target_qval': [],\n",
        "            'max_predicted_qval': [],\n",
        "            'max_target_qval': [],\n",
        "            'loss': [],\n",
        "            'steps': []\n",
        "        }\n",
        "        \n",
        "        self.with_log = with_log\n",
        "        self.loss_log_interval = loss_log_interval\n",
        "\n",
        "    def prepare_torch(self, statespace_size: int):\n",
        "        \"\"\"\n",
        "        Prepare the PyTorch model for DQL.\n",
        "        \"\"\"\n",
        "        l1 = statespace_size\n",
        "        l2 = 150\n",
        "        l3 = 100\n",
        "        l4 = self.action_space_size\n",
        "        model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(l1, l2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(l2, l3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(l3, l4)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self) -> None:\n",
        "        \"\"\"\n",
        "        Copy weights from the prediction network to the target network.\n",
        "        \"\"\"\n",
        "        # self.target_model = deepcopy(self.model)\n",
        "        target_net_state_dict = self.target_model.state_dict()\n",
        "        policy_net_state_dict = self.model.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
        "        self.target_model.load_state_dict(target_net_state_dict)\n",
        "\n",
        "    def select_action(self, state: np.ndarray, available_actions: List[Action], is_test: bool = False) -> tuple[Action, bool, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Select an action using an Îµ-greedy policy.\n",
        "        \n",
        "        second return val for is_greedy\n",
        "        chosen_action, is_greedy, q values for all actions\n",
        "        \"\"\"\n",
        "        qvals = self.get_qvals(state)\n",
        "        if not is_test and random.random() < self.epsilon:\n",
        "            return random.choice(available_actions), False, qvals\n",
        "        else:\n",
        "            # Filter Q-values to only consider available actions\n",
        "            valid_qvals = [qvals[action.value] for action in available_actions]\n",
        "            return available_actions[np.argmax(valid_qvals)], True, qvals\n",
        "        \n",
        "    def get_qvals(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get Q-values for a given state from the prediction network.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)  # Convert to tensor\n",
        "        with torch.no_grad():\n",
        "            qvals_tensor = self.model(state_tensor)\n",
        "        return qvals_tensor.detach().numpy()[0]\n",
        "\n",
        "    def get_maxQ(self, state: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Get the maximum Q-value for a given state from the target network.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)  # Convert to tensor\n",
        "        with torch.no_grad():\n",
        "            max_qval_tensor = torch.max(self.target_model(state_tensor))\n",
        "        return max_qval_tensor.item()\n",
        "    \n",
        "    def get_double_q(self, state: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the Double DQN target value for a given state.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            best_action_index = torch.argmax(self.model(state_tensor)[0]).item()\n",
        "            max_qval = self.target_model(state_tensor)[0][best_action_index].item()\n",
        "            \n",
        "        return max_qval\n",
        "\n",
        "    def train_one_step(self, states: List[np.ndarray], actions: List[int], targets: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Perform a single training step on the prediction network.\n",
        "        \"\"\"\n",
        "        # Convert states, actions, and targets to tensors\n",
        "        state_tensors = torch.cat([torch.from_numpy(s).float().unsqueeze(0) for s in states])\n",
        "        action_tensors = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
        "        target_tensors = torch.tensor(targets, dtype=torch.float)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        qvals = self.model(state_tensors).gather(1, action_tensors).squeeze()\n",
        "        losses = self.loss_fn(qvals, target_tensors)\n",
        "        loss = losses.mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        \n",
        "\n",
        "        # Logging the metrics internally\n",
        "        if self.with_log and self.steps % self.loss_log_interval == 0:\n",
        "            avg_predicted_qval = qvals.mean().item()\n",
        "            avg_target_qval = target_tensors.mean().item()\n",
        "            max_predicted_qval = qvals.max().item()\n",
        "            max_target_qval = target_tensors.max().item()\n",
        "\n",
        "            # Append to the internal tracking lists\n",
        "            self.logged_data['avg_predicted_qval'].append(avg_predicted_qval)\n",
        "            self.logged_data['avg_target_qval'].append(avg_target_qval)\n",
        "            self.logged_data['max_predicted_qval'].append(max_predicted_qval)\n",
        "            self.logged_data['max_target_qval'].append(max_target_qval)\n",
        "            self.logged_data['loss'].append(loss.item())\n",
        "            self.logged_data['steps'].append(self.steps)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            self.replay_buffer.update_priorities(losses.detach().numpy())  # TODO: maybe using l1 better (at least original paper uses l1)\n",
        "            \n",
        "        return loss.item()\n",
        "\n",
        "    def replay(self) -> None:\n",
        "        \"\"\"\n",
        "        Train the model using experience replay.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer.buffer) < self.min_replay_memory_size:\n",
        "            return\n",
        "        \n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample_batch(self.batch_size)\n",
        "\n",
        "        # Compute targets\n",
        "        targets = []\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                targets.append(rewards[i])\n",
        "            else:\n",
        "                # max_future_q = self.get_maxQ(next_states[i])\n",
        "                max_future_q = self.get_double_q(next_states[i])\n",
        "                targets.append(rewards[i] + self.discount_rate * max_future_q)\n",
        "\n",
        "        self.steps += 1\n",
        "        # Train the model\n",
        "        loss = self.train_one_step(states, actions, targets)\n",
        "\n",
        "        # TODO: plot loss\n",
        "\n",
        "    def save_state(self, filepath: Path | str):\n",
        "        \"\"\"Save the entire agent state, including model weights and hyperparameters.\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),  # Model weights\n",
        "            'target_model_state_dict': self.target_model.state_dict(),  # Target model weights\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),  # Optimizer state\n",
        "            'epsilon': self.epsilon,  # Epsilon value\n",
        "            'epsilon_decay': self.epsilon_decay,  # Epsilon decay rate\n",
        "            'epsilon_min': self.epsilon_min,  # Minimum epsilon\n",
        "            'discount_rate': self.discount_rate,  # Discount factor\n",
        "            'replay_buffer': self.replay_buffer,  # replay_buffer\n",
        "            'steps': self.steps,  # Steps to update target network\n",
        "            'random_state': random.getstate(),  # Python random state\n",
        "            'numpy_random_state': np.random.get_state(),  # Numpy random state\n",
        "        }, filepath)\n",
        "\n",
        "    def load_state(self, filepath):\n",
        "        \"\"\"Load the entire agent state, including model weights and hyperparameters.\"\"\"\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights\n",
        "        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])  # Load target model weights\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Restore optimizer state\n",
        "        self.epsilon = checkpoint['epsilon']  # Restore epsilon value\n",
        "        self.epsilon_decay = checkpoint['epsilon_decay']  # Restore epsilon decay rate\n",
        "        self.epsilon_min = checkpoint['epsilon_min']  # Restore minimum epsilon\n",
        "        self.discount_rate = checkpoint['discount_rate']  # Restore discount factor\n",
        "        self.replay_buffer = checkpoint['replay_buffer']  # Restore replay_buffer\n",
        "        self.steps = checkpoint['steps']  # Restore steps\n",
        "        random.setstate(checkpoint['random_state'])  # Restore Python random state\n",
        "        np.random.set_state(checkpoint['numpy_random_state'])  # Restore Numpy random state\n",
        "        # If using a learning rate scheduler:\n",
        "        # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])  # Restore scheduler state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Plotter class is designed to visualize and track the performance metrics of a Deep Q-Network (DQN) agent during training. It provides real-time plotting of key metrics such as the average predicted Q-values, target Q-values, and training loss over time. This visualization helps in understanding how well the DQN agent is learning and adapting its policy.\n",
        "Key Features:\n",
        "<li>Initialization: Sets up the plotting environment and creates a directory to save the generated plots.</li>\n",
        "<li>update_plot: Updates the plots dynamically with the latest logged data, including average Q-values and loss, ensuring that the plots reflect the most recent training progress. </li>\n",
        "<li>save_plot: Saves the updated plots as image files in the specified directory, allowing for easy reference and comparison of the agent's learning performance at different stages of training. </li>\n",
        "The Plotter class provides a convenient way to monitor and evaluate the training process of a DQN agent visually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, save_dir: str = \"./plots\"):\n",
        "        \"\"\"\n",
        "        Initialize the Plotter with the DQNAgent instance and refresh interval.\n",
        "        \n",
        "        :param agent: Instance of the DQNAgent to fetch data from.\n",
        "        :param refresh_interval: Time interval (in milliseconds) to refresh the plot.\n",
        "        :param save_dir: Directory where plots will be saved.\n",
        "        \"\"\"\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # Create the save directory if it doesn't exist\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        # Set up the plot\n",
        "        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "        self.fig.suptitle('DQN Agent Metrics Tracking')\n",
        "\n",
        "    def update_plot(self,logged_data):\n",
        "        \"\"\"\n",
        "        Update the plot with new data.\n",
        "        \"\"\"\n",
        "        # Fetch the latest metrics from the agent\n",
        "        \n",
        "        data = logged_data\n",
        "        \n",
        "        \n",
        "        # Clear the previous plots\n",
        "        self.ax1.clear()\n",
        "        self.ax2.clear()\n",
        "\n",
        "        # Plot average predicted Q-values and target Q-values\n",
        "        if data['avg_predicted_qval'] and data['avg_target_qval']:\n",
        "            self.ax1.plot(data['steps'], data['avg_predicted_qval'], label='Avg Predicted Q-Value', color='blue')\n",
        "            self.ax1.plot(data['steps'], data['avg_target_qval'], label='Avg Target Q-Value', color='green')\n",
        "            self.ax1.set_xlabel('Step')\n",
        "            self.ax1.set_ylabel('Q-Value')\n",
        "            self.ax1.set_title('Average Q-Values over Steps')\n",
        "            self.ax1.legend()\n",
        "\n",
        "        # Plot loss\n",
        "        if data['loss']:\n",
        "            self.ax2.plot(data['steps'], data['loss'], label='Loss', color='red')\n",
        "            self.ax2.set_xlabel('Step')\n",
        "            self.ax2.set_ylabel('Loss')\n",
        "            self.ax2.set_title('Training Loss over Steps')\n",
        "            self.ax2.legend()\n",
        "\n",
        "        # Refresh the plot\n",
        "        self.fig.canvas.draw()\n",
        "\n",
        "        # Save the plot to a file\n",
        "        self.save_plot()\n",
        "\n",
        "    def save_plot(self):\n",
        "        \"\"\"\n",
        "        Save the current plot to a file in the specified directory.\n",
        "        \"\"\"\n",
        "        filename = os.path.join(self.save_dir, f\"agent_metrics.png\")\n",
        "        self.fig.savefig(filename)\n",
        "        print(f\"Plot saved to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cyt3tcaEm8M"
      },
      "source": [
        "**<p>Trainer that will be training the model</p>**\n",
        "\n",
        "Learning Visualization:\n",
        "\n",
        "*   Early Stages: In the early stages of training, the agent performs random actions to explore the environment, which can result in the agent moving further away from the goal positions.\n",
        "*   Later Stages: After training for many episodes, the agent learns to make optimal decisions. The visualization shows the agent making quick and direct movements towards the item, collecting it, and then moving efficiently to the final goal position (G).\n",
        "\n",
        "<p>plot_rewards(): Visualizes total rewards per episode to show learning progress.</p>\n",
        "<p>plot_epsilon_decay(): Shows how the exploration rate (epsilon) decreases over episodes, indicating the shift from exploration to exploitation.<p>\n",
        "<p>plot_validation_scores(): plots the validation scores</p>\n",
        "<p>Also in plots folder: we get a plot of the networks metrics every 10 episodes and final ending metrics</p>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "collapsed": true,
        "id": "UXUXzzgaK4QD",
        "outputId": "bfd43596-9702-4f4b-f49b-77ef30141f41"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        agent: DQNAgent, \n",
        "        environment: Assignment2Environment, \n",
        "        with_log: bool = False, \n",
        "        log_step: int = 100, \n",
        "        update_target_episodes: int = 20, \n",
        "        num_validation_episodes: int = 30, \n",
        "        save_checkpoint_interval: int = 50,  # in episodes\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Trainer with the DQN agent and environment.\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.environment = environment\n",
        "        \n",
        "        self.update_target_episodes = update_target_episodes\n",
        "        \n",
        "        self.episode_rewards: list[float] = []\n",
        "        self.validation_scores: list[float] = []\n",
        "        \n",
        "        self.with_log = with_log\n",
        "        self.global_step = 0\n",
        "        self.log_step = log_step\n",
        "        self.num_validation_episodes = num_validation_episodes\n",
        "        \n",
        "        self.save_checkpoint_interval = save_checkpoint_interval\n",
        "\n",
        "        # Initialize the Plotter\n",
        "        self.plotter = Plotter(save_dir=\"./plots\")\n",
        "\n",
        "\n",
        "    def train_one_episode(self, epoch_idx: int) -> None:\n",
        "        \"\"\"\n",
        "        Conducts training for a single episode.\n",
        "        \"\"\"\n",
        "        self.environment.initialize_for_new_episode()\n",
        "\n",
        "        current_state = self.environment.get_state()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        while not done:\n",
        "            state_array = self.state_to_array(current_state)\n",
        "            available_actions = self.environment.get_available_actions(current_state)\n",
        "            action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions)\n",
        "            reward, next_state = self.environment.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "            # print(f\"S_t={current_state}, A={action.name}, R={reward}, S_t+1={next_state}\")\n",
        "            next_state_array = self.state_to_array(next_state)\n",
        "            done = self.environment.is_goal_state(next_state)\n",
        "            total_reward += reward\n",
        "            \n",
        "            self.agent.replay_buffer.remember((state_array, action.value, reward, next_state_array, done))\n",
        "            self.agent.replay()  # maybe train inside\n",
        "            \n",
        "            current_state = next_state\n",
        "            step_count += 1\n",
        "            self.global_step += 1\n",
        "\n",
        "        # decrease exploration over time\n",
        "        self.agent.epsilon = max(self.agent.epsilon_min, self.agent.epsilon * self.agent.epsilon_decay)\n",
        "        self.episode_rewards.append(total_reward)\n",
        "\n",
        "    def state_to_array(self, state: Assignment2State) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a State object into a numpy array suitable for input to the DQN.\n",
        "        \"\"\"\n",
        "        # Convert Assignment2State to array\n",
        "        return np.array([\n",
        "            *state.agent_location,  # Agent's (x, y) location\n",
        "            *state.item_location,   # Item's (x, y) location\n",
        "            float(state.has_item),  # 1 if agent has item, 0 otherwise\n",
        "            *state.goal_location,   # Goal's (x, y) location\n",
        "            *state.goal_direction,  # Direction to goal (dx, dy)\n",
        "            *state.item_direction   # Direction to item (dx, dy)\n",
        "        ])\n",
        "\n",
        "    def train(self, num_episodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Train the agent across multiple episodes.\n",
        "        \"\"\"\n",
        "        num_nn_passes = 0\n",
        "        current_best_validation_score = -float('inf')\n",
        "\n",
        "        for episode in range(1, num_episodes+1):\n",
        "            print(f\"Starting Episode {episode + 1}\")\n",
        "            self.train_one_episode(episode)\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                self.plotter.update_plot(self.agent.logged_data)\n",
        "\n",
        "            if episode % self.update_target_episodes == 0:\n",
        "                self.agent.update_target_network()\n",
        "                if self.with_log:\n",
        "                    print(\"Target network updated\")\n",
        "            print(f\"Episode {episode + 1} completed. Epsilon: {self.agent.epsilon:.4f}\")\n",
        "            if self.agent.steps != num_nn_passes:\n",
        "                validation_score = self.validate(episode)\n",
        "                self.validation_scores.append(validation_score)\n",
        "                if validation_score > current_best_validation_score:\n",
        "                    print(f\"New best validation score: {validation_score}\")\n",
        "                    current_best_validation_score = validation_score\n",
        "                    self.save_agent(episode)\n",
        "                self.visualize_sample_episode()\n",
        "                num_nn_passes = self.agent.steps\n",
        "            if episode % self.save_checkpoint_interval == 0:\n",
        "                self.save_agent(episode)\n",
        "                \n",
        "        \n",
        "        # Plot and save the rewards and epsilon decay after training is complete\n",
        "        self.plot_rewards(save=True, filename='reward_plot.png')\n",
        "        self.plot_epsilon_decay(num_episodes, save=True, filename='epsilon_decay_plot.png')\n",
        "        self.plot_validation_scores(save=True, filename='validation_score_plot.png')  # Plot validation scores\n",
        "        self.plotter.update_plot(self.agent.logged_data)\n",
        "\n",
        "    def visualize_sample_episode(self) -> None:\n",
        "        sample_env = Assignment2Environment(n=4, with_animation=True)\n",
        "        sample_env.initialize_for_new_episode()\n",
        "        current_state = sample_env.get_state()\n",
        "        start_time = time.time()\n",
        "        done = False\n",
        "        \n",
        "        prev_state = None\n",
        "        \n",
        "        while not done and time.time() - start_time < 1*20:\n",
        "            state_array = self.state_to_array(current_state)\n",
        "            available_actions = sample_env.get_available_actions(current_state)\n",
        "            action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions, is_test=True)\n",
        "            reward, next_state = sample_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "            done = sample_env.is_goal_state(next_state)\n",
        "            \n",
        "            # check for three-step cycle and stop early\n",
        "            if next_state == prev_state:\n",
        "                print(\"cycle detected... breaking\")\n",
        "                break\n",
        "            prev_state = current_state\n",
        "            current_state = next_state\n",
        "        \n",
        "        plt.close('all')\n",
        "\n",
        "    def validate(self, current_episode_index: int):\n",
        "        calulated_scores = []\n",
        "        for _ in range(self.num_validation_episodes):\n",
        "            sample_env = Assignment2Environment(n=4, with_animation=False)\n",
        "            sample_env.initialize_for_new_episode()\n",
        "            sample_env.current_sub_environment.agent.has_item = False # metric assumes that agent starts without item\n",
        "            current_state = sample_env.get_state()\n",
        "            start_time = time.time()\n",
        "            done = False\n",
        "            start_location = sample_env.current_sub_environment.agent.get_location()\n",
        "            item_location = sample_env.current_sub_environment.item.get_location()\n",
        "            goal_location = sample_env.current_sub_environment.goal_location\n",
        "            \n",
        "            prev_state = None\n",
        "            predicted_steps = 0\n",
        "            while not done:\n",
        "                if time.time() - start_time > 20:\n",
        "                    predicted_steps = 0\n",
        "                    break\n",
        "                state_array = self.state_to_array(current_state)\n",
        "                available_actions = sample_env.get_available_actions(current_state)\n",
        "                action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions, is_test=True)\n",
        "                reward, next_state = sample_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "                done = sample_env.is_goal_state(next_state)\n",
        "                \n",
        "                # check for three-step cycle and stop early\n",
        "                if next_state == prev_state:\n",
        "                    predicted_steps = 0\n",
        "                    break\n",
        "                prev_state = current_state\n",
        "                current_state = next_state\n",
        "                predicted_steps += 1\n",
        "            calulated_scores.append(calculate_metrics_score(predicted_steps, start_location, item_location, goal_location))\n",
        "        \n",
        "        result = sum(calulated_scores) / self.num_validation_episodes\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_agent(self, episode_index: int) -> None:\n",
        "        save_path = Path(f\"checkpoints/episode_{episode_index}.pt\")\n",
        "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.agent.save_state(save_path)\n",
        "\n",
        "    def plot_rewards(self, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the total reward earned per episode.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.episode_rewards, label='Total Reward per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "        plt.title('Reward Earned per Episode')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Reward plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def plot_validation_scores(self, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the validation scores over episodes.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.validation_scores, label='Validation Score per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Validation Score')\n",
        "        plt.title('Validation Score Over Episodes')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Validation score plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def plot_epsilon_decay(self, num_episodes: int, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the epsilon decay over episodes.\n",
        "        \"\"\"\n",
        "        epsilons = [max(self.agent.epsilon_min, self.agent.epsilon * (self.agent.epsilon_decay ** i)) for i in range(num_episodes)]\n",
        "        \n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(range(num_episodes), epsilons, label='Epsilon Decay')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.title('Epsilon Decay over Episodes')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Epsilon decay plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBiEKEQjkSgC"
      },
      "source": [
        "### Metric:\n",
        "$$ \\frac{1}{number\\ of\\ episodes}\\sum_{item\\ location}\\sum_{agent\\ location}\\sum_{goal\\ location}{\\frac{M(agent\\ location, item\\ location) + 1 + M(item\\ location, goal\\ location)}{number\\ of\\ steps\\ taken}} $$\n",
        "where M represents Manhattan distance\n",
        "\n",
        "  We began by training our DQN for all possible item locations and goal locations. After training, we evaluated the agent's performance by calculating the ratio of the shortest possible distance (using Manhattan distance) to the actual number of steps taken by the agent from the start position, through the item, and finally reaching the goal (shortest distance / actual distance taken by the agent). This calculation was done for all possible agent, item and goal locations within the environment. A ratio of 1 indicates that the actual path matches the shortest path, while a ratio less than 1 suggests a longer path was taken. By averaging these ratios across all possible scenarios, we can obtain the metric.\n",
        "  \n",
        "  We obtained an average value greater than xxx, ndicating that our DQN effectively learns the optimal paths in the environment. Note that 1 in the metric represents additional step to pick up item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PopRHtiPD2sV"
      },
      "outputs": [],
      "source": [
        "def calculate_manhattan_distance(start_location: tuple[int, int], goal_location: tuple[int, int]) -> int:\n",
        "    \"\"\"\n",
        "    Calculates the Manhattan distance between two points.\n",
        "    \"\"\"\n",
        "    start_x, start_y = start_location\n",
        "    goal_x, goal_y = goal_location\n",
        "    return abs(start_x - goal_x) + abs(start_y - goal_y)\n",
        "\n",
        "def calculate_metrics_score(predicted_distance: int, start_location: tuple[int, int], item_location: tuple[int, int], goal_location: tuple[int, int]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the proportion of the distance to the shortest distance.\n",
        "    \"\"\"\n",
        "    # Calculate shortest distance from start to item to goal\n",
        "    shortest_distance = (\n",
        "        calculate_manhattan_distance(start_location, item_location)\n",
        "        + 1\n",
        "        + calculate_manhattan_distance(item_location, goal_location)\n",
        "    )\n",
        "    return (shortest_distance / predicted_distance) if predicted_distance != 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kqwlt3kB7-IR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Episode 2\n",
            "Episode 2 completed. Epsilon: 1.0000\n",
            "Starting Episode 3\n",
            "Episode 3 completed. Epsilon: 0.9999\n",
            "Starting Episode 4\n",
            "Episode 4 completed. Epsilon: 0.9999\n",
            "Starting Episode 5\n",
            "Episode 5 completed. Epsilon: 0.9999\n",
            "Starting Episode 6\n",
            "Episode 6 completed. Epsilon: 0.9999\n",
            "Starting Episode 7\n",
            "Episode 7 completed. Epsilon: 0.9998\n",
            "Starting Episode 8\n",
            "Episode 8 completed. Epsilon: 0.9998\n",
            "Starting Episode 9\n",
            "Episode 9 completed. Epsilon: 0.9998\n",
            "Starting Episode 10\n",
            "Episode 10 completed. Epsilon: 0.9997\n",
            "Starting Episode 11\n",
            "Plot saved to ./plots\\agent_metrics.png\n",
            "Episode 11 completed. Epsilon: 0.9997\n",
            "Starting Episode 12\n",
            "Episode 12 completed. Epsilon: 0.9997\n",
            "Starting Episode 13\n",
            "Episode 13 completed. Epsilon: 0.9996\n",
            "Starting Episode 14\n",
            "Episode 14 completed. Epsilon: 0.9996\n",
            "Starting Episode 15\n",
            "Episode 15 completed. Epsilon: 0.9996\n",
            "Starting Episode 16\n",
            "Episode 16 completed. Epsilon: 0.9996\n",
            "Starting Episode 17\n",
            "Episode 17 completed. Epsilon: 0.9995\n",
            "New best validation score: 0.0\n",
            "cycle detected... breaking\n",
            "Starting Episode 18\n",
            "Episode 18 completed. Epsilon: 0.9995\n",
            "cycle detected... breaking\n",
            "Starting Episode 19\n",
            "Episode 19 completed. Epsilon: 0.9995\n",
            "cycle detected... breaking\n",
            "Starting Episode 20\n",
            "Episode 20 completed. Epsilon: 0.9994\n",
            "cycle detected... breaking\n",
            "Starting Episode 21\n",
            "Plot saved to ./plots\\agent_metrics.png\n",
            "Target network updated\n",
            "Episode 21 completed. Epsilon: 0.9994\n",
            "cycle detected... breaking\n",
            "Starting Episode 22\n",
            "Episode 22 completed. Epsilon: 0.9994\n",
            "cycle detected... breaking\n",
            "Starting Episode 23\n",
            "Episode 23 completed. Epsilon: 0.9993\n",
            "cycle detected... breaking\n",
            "Starting Episode 24\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m evl \u001b[38;5;241m=\u001b[39m Evaluation()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Training DQN\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[43mevl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_dqn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Load DQN model\u001b[39;00m\n\u001b[0;32m    147\u001b[0m current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;66;03m# get current path\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mEvaluation.run_dqn_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03mTrains DQN agent in the environment and save the states.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn_agent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn_envs, with_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m110\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn_agent\u001b[38;5;241m.\u001b[39msave_state(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_dqn.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[8], line 94\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplotter\u001b[38;5;241m.\u001b[39mupdate_plot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mlogged_data)\n",
            "Cell \u001b[1;32mIn[8], line 60\u001b[0m, in \u001b[0;36mTrainer.train_one_episode\u001b[1;34m(self, epoch_idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mis_goal_state(next_state)\n\u001b[0;32m     58\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mreplay()  \u001b[38;5;66;03m# maybe train inside\u001b[39;00m\n\u001b[0;32m     63\u001b[0m current_state \u001b[38;5;241m=\u001b[39m next_state\n",
            "Cell \u001b[1;32mIn[5], line 61\u001b[0m, in \u001b[0;36mPrioritizedExperienceBuffer.remember\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mappend(experience)\n\u001b[1;32m---> 61\u001b[0m max_priority \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpriorities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# max to ensure newly added experience has the highest priority\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m max_priority\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, n=4) -> None:\n",
        "        self.n = n\n",
        "        self.dqn_envs = Assignment2Environment(n=4, with_animation=False)\n",
        "        self.dqn_agent = DQNAgent(with_log=True)\n",
        "\n",
        "    def run_dqn_train(self):\n",
        "        \"\"\"\n",
        "        Trains DQN agent in the environment and save the states.\n",
        "        \"\"\"\n",
        "        trainer = Trainer(self.dqn_agent, self.dqn_envs, with_log=True)\n",
        "        trainer.train(num_episodes=300)\n",
        "        self.dqn_agent.save_state(\"trained_dqn.pth\")\n",
        "\n",
        "    def load_trained_dqn(self, path: str):\n",
        "        \"\"\"\n",
        "        Load the saved DQN\n",
        "        \"\"\"\n",
        "        self.dqn_agent.load_state(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_grid_location_list(max_x: int, max_y) -> list[tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Generate the grid location list for all possible cases\n",
        "        \"\"\"\n",
        "        return [(i, j) for i in range(max_x) for j in range(max_y)]\n",
        "\n",
        "    def state_to_array(self, state: State) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a State object into a numpy array suitable for input to the DQN.\n",
        "        \"\"\"\n",
        "        # Check if the state is an instance of Assignment2State\n",
        "        if isinstance(state, Assignment2State):\n",
        "            # Convert Assignment2State to array\n",
        "            state_array = np.array([\n",
        "                *state.agent_location,  # Agent's (x, y) location\n",
        "                *state.item_location,   # Item's (x, y) location\n",
        "                float(state.has_item),  # 1 if agent has item, 0 otherwise\n",
        "                *state.goal_location,   # Goal's (x, y) location\n",
        "                *state.goal_direction,  # Direction to goal (dx, dy)\n",
        "                *state.item_direction   # Direction to item (dx, dy)\n",
        "            ])\n",
        "        else:\n",
        "            # Convert basic State to array (without goal-related information)\n",
        "            state_array = np.array([\n",
        "                *state.agent_location,  # Agent's (x, y) location\n",
        "                *state.item_location,   # Item's (x, y) location\n",
        "                float(state.has_item)   # 1 if agent has item, 0 otherwise\n",
        "            ])\n",
        "\n",
        "        # Ensure the state array matches the input size of the neural network\n",
        "        if len(state_array) != 11:\n",
        "            print(f\"Warning: State array length mismatch. Expected 11, got {len(state_array)}. Padding with zeros.\")\n",
        "            state_array = np.pad(state_array, (0, 11 - len(state_array)), 'constant')\n",
        "        return state_array\n",
        "\n",
        "    def dqn_performance_test(self):\n",
        "        \"\"\"\n",
        "        Conducts a performance test for DQN. The maximum of the score is 1.\n",
        "        \"\"\"\n",
        "        num_episodes = 0\n",
        "        total_score = 0\n",
        "\n",
        "        # Loop over all environments in DQN environment\n",
        "        for i, _ in tqdm(enumerate(self.dqn_envs.environments)):\n",
        "            for agent_location in tqdm(self.generate_grid_location_list(self.n, self.n)):\n",
        "                # Initialize episode with a given agent location\n",
        "                self.dqn_envs.initialize_for_new_episode(agent_location=agent_location, env_index=i)\n",
        "\n",
        "                # Ensure agent location is not same place with item and goal\n",
        "                if agent_location == self.dqn_envs.current_sub_environment.item.get_location() or agent_location == self.dqn_envs.current_sub_environment.goal_location:\n",
        "                    continue\n",
        "\n",
        "                # Metric assumes that agent starts without item\n",
        "                self.dqn_envs.current_sub_environment.agent.has_item = False\n",
        "\n",
        "                # Get start, item, and goal location to calcurate distance\n",
        "                start_location = self.dqn_envs.current_sub_environment.agent.get_location()\n",
        "                item_location = self.dqn_envs.current_sub_environment.item.get_location()\n",
        "                goal_location = self.dqn_envs.current_sub_environment.goal_location\n",
        "\n",
        "                current_state = self.dqn_envs.get_state() # get current state\n",
        "                start_time = time.time() # to keeps track time\n",
        "                predicted_steps = 0 # count the number of actual steps taken\n",
        "                done = False # for one environment\n",
        "                is_break = False # to keep track the break\n",
        "\n",
        "                while not done:\n",
        "                    # Break if it takes more than 20 seconds\n",
        "                    if time.time() - start_time > 20:\n",
        "                        is_break = True\n",
        "                        break\n",
        "                    state_array = self.state_to_array(current_state) # get the states in array format\n",
        "                    available_actions = self.dqn_envs.get_available_actions(current_state) # get available actions\n",
        "                    action, is_greedy, all_qvals = self.dqn_agent.select_action(state_array, available_actions, is_test=True)\n",
        "                    reward, next_state = self.dqn_envs.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals) # get next state\n",
        "                    done = self.dqn_envs.is_goal_state(next_state) # Check if it is goal position\n",
        "                    current_state = next_state # update current state\n",
        "                    predicted_steps += 1\n",
        "\n",
        "                if not is_break:\n",
        "                    # calculate the metrics score\n",
        "                    total_score += calculate_metrics_score(predicted_steps, start_location, item_location, goal_location)\n",
        "                    num_episodes += 1 # increase the episode\n",
        "\n",
        "            # Return the average score across all possible tests\n",
        "            return (total_score / num_episodes) if num_episodes != 0 else 0\n",
        "\n",
        "    def visualize_dqn(self, num_of_vis: int = 5) -> None:\n",
        "        \"\"\"\n",
        "        Visualize the path after trained for given times\n",
        "        \"\"\"\n",
        "        for _ in (0, num_of_vis):\n",
        "            self.dqn_envs.set_with_animation(True) # Set the animation True\n",
        "            self.dqn_envs.initialize_for_new_episode()\n",
        "            self.dqn_envs.current_sub_environment.agent.has_item = False # Assumes that agent starts without item\n",
        "\n",
        "            current_state = self.dqn_envs.get_state()\n",
        "            start_time = time.time() # Keep track time\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # If it takes more than 20 seconds to reach the goal, break the loop\n",
        "                if time.time() - start_time > 20:\n",
        "                    break\n",
        "                state_array = self.state_to_array(current_state) # get the states in array format\n",
        "                available_actions = self.dqn_envs.get_available_actions(current_state) # get available actions\n",
        "                action, is_greedy, all_qvals = self.dqn_agent.select_action(state_array, available_actions, is_test=True)\n",
        "                reward, next_state = self.dqn_envs.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals) # get next state\n",
        "                done = self.dqn_envs.is_goal_state(next_state) # Check if it is goal position\n",
        "                current_state = next_state # update current state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # DQN\n",
        "    evl = Evaluation()\n",
        "\n",
        "    # Training DQN\n",
        "    evl.run_dqn_train()\n",
        "\n",
        "    # Load DQN model\n",
        "    current_path = os.getcwd() # get current path\n",
        "    saved_path = current_path+'/trained_dqn.pth'\n",
        "    evl.load_trained_dqn(saved_path)\n",
        "\n",
        "    # Conduct the performance test\n",
        "    average_score = evl.dqn_performance_test()\n",
        "    print(f\"Average performance score (1 is the best): {average_score:.4f}\")\n",
        "\n",
        "    # Visualize randomly the environments and show the steps of the agent\n",
        "    evl.visualize_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBzklVnunW2_"
      },
      "source": [
        "**<p>Hyperparameter Tuning</p>**\n",
        "\n",
        "<p> We performed hyperparameter tuning for the DQN agent using library Optuna. An objective function defined to evaluate different sets of hyperparameters and optimize them to maximize the DQN agent's total reward.</p>\n",
        "\n",
        "<p>We set a maximum amount of time allowed of 10 seconds for each trial while running optimisation.</p>\n",
        "\n",
        "<p>Below are the sets of hyperparameters used in tuning:</p>\n",
        "\n",
        "<ol>\n",
        "<li>Optimizer learning rate <i>(alpha)</i> from range of 0.995 to 0.999</li>\n",
        "<li>Discount factor for future rewards <i>(discount_rate)</i> from range of 0.95 to 0.975</li>\n",
        "<li>Exploration rate <i>(epsilon)</i> with a set of 0.2, 0.5, 0.4 and 0.6</li>\n",
        "<li>Maximum size of replay memory <i>(replay_memory_size)</i> from range of 1000 to 5000 </li>\n",
        "<li>Batch size for experience replay <i>(batch_size)</i> with a set of 16, 32, 64, 128, 256 </li>\n",
        "<li>Time penalty <i>(time_penalty)</i>  from range of -10 to -1</li>\n",
        "<li>Penalty for reach goal without item <i>(goal_no_item_penalty)</i> from range of -500 to -100</li>\n",
        "<li>Penalty for revisiting the item location <i>(item_revisit_penalty)</i> from range of -200 to -50</li>\n",
        "<li>Reward for collecting the item <i>(item_state_reward)</i> from range of 100 to 200</li>\n",
        "<li>Reward for reaching the goal with item collected <i>(goal_state_reward)</i> from range of 300 to 600</li>\n",
        "<li>Number of epsiodes to train the model <i>(num_episodes)</i> from range of 100 to 600</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "<p>We then initialised the environment and agent with the sets of hyperparameters and start training loop.</p>\n",
        "\n",
        "<p>We call Optuna to start the optimsation process with 50 number of trials. The best hyperparameters and total rewards are printed, and saved into YAML file. The Optimisation history is plotted to visualize how the total reward improved over trials</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLgTXWN4nVvn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import optuna\n",
        "import time\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "TIME_LIMIT = 10\n",
        "\n",
        "\n",
        "class Tuning:\n",
        "    def __init__(self, time_limit: int = TIME_LIMIT) -> None:\n",
        "        self.study = optuna.create_study(direction='maximize') # Create an Optuna study\n",
        "        self.time_limit = time_limit\n",
        "\n",
        "    def objective(self, trial: optuna.Trial) -> float:\n",
        "        # Hyperparameters to optimize\n",
        "        alpha = trial.suggest_float('alpha', 0.995, 0.999)\n",
        "        discount_rate = trial.suggest_float('discount_rate', 0.95, 0.975)\n",
        "        epsilon = trial.suggest_categorical('epsilon', [0.2, 0.35, 0.45, 0.6])\n",
        "        replay_memory_size = trial.suggest_int('replay_memory_size', 1000, 5000)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
        "        time_penalty = trial.suggest_int('time_penalty', -10, -1)\n",
        "        goal_no_item_penalty = trial.suggest_int('goal_no_item_penalty', -500, -100)\n",
        "        item_revisit_penalty = trial.suggest_int('item_revisit_penalty', -200, -50)\n",
        "        item_state_reward = trial.suggest_int('item_state_reward', 100, 200)\n",
        "        goal_state_reward = trial.suggest_int('goal_state_reward', 300, 600)\n",
        "        num_episodes = trial.suggest_int('num_episodes', 100, 600)\n",
        "\n",
        "        # Initialize Assignment2Environment\n",
        "        tune_env = Assignment2Environment(\n",
        "        n=4,  # Grid size\n",
        "        time_penalty=time_penalty,\n",
        "        goal_no_item_penalty=goal_no_item_penalty,\n",
        "        item_revisit_penalty=item_revisit_penalty,\n",
        "        item_state_reward=item_state_reward,\n",
        "        goal_state_reward=goal_state_reward,\n",
        "        direction_reward_multiplier=1,\n",
        "        with_animation=False\n",
        "        )\n",
        "\n",
        "        # Initialize DQNAgent\n",
        "        tune_agent = DQNAgent(\n",
        "        alpha=alpha,\n",
        "        discount_rate=discount_rate,\n",
        "        epsilon=epsilon,\n",
        "        replay_memory_size=replay_memory_size,\n",
        "        batch_size=batch_size,\n",
        "        )\n",
        "\n",
        "        # num_episodes = 200 # define episodes\n",
        "        tune_trainer = Trainer(tune_agent, tune_env)\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            tune_env.initialize_for_new_episode() # Initialize the environment for a new episode\n",
        "            current_state = tune_env.get_state()  # Get state from current sub-environment\n",
        "            total_reward = 0  # Track total reward for the episode\n",
        "            start_time = time.time()  # Record the start time of the episode\n",
        "\n",
        "            while not tune_env.is_goal_state(current_state):\n",
        "                # Check if time limit is exceeded\n",
        "                elapsed_time = time.time() - start_time\n",
        "                if elapsed_time > self.time_limit:\n",
        "                    break\n",
        "\n",
        "                # Convert the current state to a numpy array for input to the neural network\n",
        "                state_array = tune_trainer.state_to_array(current_state)\n",
        "\n",
        "                # Retrieve available actions from the current sub-environment\n",
        "                available_actions = tune_env.get_available_actions(current_state)\n",
        "\n",
        "                # Select an action using the agent's Îµ-greedy policy\n",
        "                action, is_greedy, all_qvals = tune_agent.select_action(state_array, available_actions)\n",
        "\n",
        "                # Execute the action in the current sub-environment, receive reward and next state\n",
        "                reward, next_state = tune_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "\n",
        "\n",
        "                # Add the reward to the total reward for this episode\n",
        "                total_reward += reward\n",
        "\n",
        "                # Convert the next state to a numpy array\n",
        "                next_state_array = tune_trainer.state_to_array(next_state)\n",
        "\n",
        "                # Store experience in the agent's replay memory\n",
        "                tune_agent.replay_buffer.remember((state_array, action.value, reward, next_state_array, tune_env.is_goal_state(next_state)))\n",
        "\n",
        "                # Learn from experiences using experience replay\n",
        "                tune_agent.replay()\n",
        "\n",
        "                # Move to the next state\n",
        "                current_state = next_state\n",
        "\n",
        "            # decrease exploration over time\n",
        "            tune_agent.epsilon = max(tune_agent.epsilon_min, tune_agent.epsilon * tune_agent.epsilon_decay)\n",
        "            # Store total reward of the episode\n",
        "            tune_trainer.episode_rewards.append(total_reward)\n",
        "\n",
        "            # Prune the trial early if it's performing poorly\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "\n",
        "    def run_hyperparameter_tuning(self):\n",
        "        '''\n",
        "        Run hyperparameter tunig and save the best parameters\n",
        "        '''\n",
        "        # Define the number of trials\n",
        "        num_trials = 50\n",
        "\n",
        "        # Optimize the objective function\n",
        "        self.study.optimize(self.objective, n_trials=num_trials, show_progress_bar=True)\n",
        "\n",
        "        # Print the best hyperparameters and the best value\n",
        "        print(\"Best Hyperparameters: \", self.study.best_params)\n",
        "        print(\"Best Value: \", self.study.best_value)\n",
        "\n",
        "        # Save the best hyperparameters to a YAML file\n",
        "        with open(\"config.yml\", \"w\") as file:\n",
        "            yaml.dump(self.study.best_params, file, default_flow_style=False)\n",
        "\n",
        "    def hyperparameter_tuning_visualization(self):\n",
        "        '''\n",
        "        Visualize the optimization history\n",
        "        '''\n",
        "        optuna.visualization.plot_optimization_history(self.study)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tuning = Tuning()\n",
        "\n",
        "    # Conduct hyperparameter tuning\n",
        "    tuning.run_hyperparameter_tuning()\n",
        "\n",
        "    # Visualize the hyperparameter tuning\n",
        "    tuning.hyperparameter_tuning_visualization()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
