{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1f7VshMlilR"
      },
      "source": [
        "**<p>Action:</p>**\n",
        "\n",
        "We have 5 actions: going left, right, down, up, and collecting an item. The action other than collecting items is generally available to all states (except where it’s inappropriate e.g. at the right-top corner, only available actions should be left and down actions). The action to collect the item is only available at the first time visiting the item location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GhB0hbA_cf2"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Action(Enum):\n",
        "    # NOTE: QValue matrix used these action values as their indices\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "    DOWN = 2\n",
        "    UP = 3\n",
        "\n",
        "    # actions when agent just got the item and is moving to item_reached state\n",
        "    COLLECT = 4  # goes to item reached state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpKvy3qF_cf4"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TkAgg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<p>State</p>**\n",
        "\n",
        "We design a state space with 5 dimensions: (agent_location_x, agent_location_y, is_item_collected, item_location_x, item_location_y). Therefore, our q-value table has a shape of (5, 5, 2, 5, 5). Some points to note:\n",
        "<ol><li>To fairly train states for all item locations, and because they are independent, we train these q-tables separately. Therefore, the agent is trained with one Q Table that consists of (agent_location_x, agent_location_y, is_item_collected) and this is repeated for environments with different pairs of (item_location_x, item_location_y).</li>\n",
        "<li>Because the item should not be at the goal position, the number of possible item locations should be 5*5 - 1 instead of 5*5. However, numpy must have the same array size across one dimension; therefore, we simply allocate 5*5 item space and then do not use the item position state corresponding to the position of the item.</li>\n",
        "<li>Technically, we only need one (agent_x, agent_y) table for all item positions after visiting the item location, because the state space after collecting the item is independent of the position of the item. However, to simplify implementation (and because we did not have time), we have a redundant (agent_x, agent_y) array for all item positions of the item-collected dimension.</li></ol>\n",
        "\n",
        "<br>\n",
        "\n",
        "**<p>Reward Structure</p>**\n",
        "\n",
        "For the reward structure, please see get_reward function of the Environment class. We give an item collection reward of 200 iff it is the first time to visit where the item is (to avoid giving a reward at second visit to the item position). We give a reward of 300 for the goal state. Note that the goal state only exists in the item_collected dimension and not in non item_collected dimension (as reaching a goal position before collecting the item should be disallowed). Other than those two cases, we give a time penalty of -1.\n",
        "\n",
        "<br>\n",
        "\n",
        "**<p>Visualization</p>**\n",
        "\n",
        "In the environment, we start by taking in 2 parameters, n (for the grid size for n x n dimensions) and with_animation which is a boolean value if we want to show animation or not.\n",
        "\n",
        "In the defined **initialize_for_new_episode** function, we create the plot using matplotlib.and call the animate function to setup the elements inside the plot.\n",
        "\n",
        "\n",
        "In the defined **animate** function,\n",
        "\n",
        "<li>Sets up the grid of size n x n.</li>\n",
        "<li>It adds in the 3 icons, A - “person”, I - “item” , G - “Goal”</li>\n",
        "<li>The above is also represented in a legend.</li>\n",
        "<li>In the grid we have it so that G is always positioned in the bottom right corner.</li>\n",
        "<li>When the agent picks up the Item the colour changes and legend will reflect that.</li>\n",
        "<li>We add a little delay of 0.5 sec, so that the changes can be made on the grid.</li>\n",
        "\n",
        "In the defined **step** function,\n",
        "\n",
        "<li>This is the function that causes the movement.</li>\n",
        "<li>The agent will choose an action.</li>\n",
        "<li>Perform the movement if possible and update its state,after this it calls the animate function to show the new state.</li>\n",
        "\n",
        "By using the visualisation we can see that in the early stages of learning the agent was performing random actions and exploring more, moving further away from the two goal positions, but after training many episodes, the agent grew smarter and makes quick movements straight towards the item, picks up the item and moves to the final goal position G.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bDXEl1kK_pC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbQcTCYf_cf4"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from abc import ABC\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.axis\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DEFAULT_TIME_PENALTY = -1\n",
        "GOAL_STATE_REWARD = 300\n",
        "DEFAULT_ITEM_REWARD = 200\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: int = 5,\n",
        "        fig: matplotlib.figure.Figure | None = None,\n",
        "        ax: matplotlib.axes.Axes | None = None,\n",
        "        item: ItemObject | None = None,\n",
        "        goal_location: tuple[int, int] = (4, 0),\n",
        "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
        "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
        "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
        "        with_animation: bool = True,\n",
        "    ) -> None:\n",
        "        self.n = n\n",
        "        self.goal_location = goal_location\n",
        "        self.time_penalty = time_penalty\n",
        "        self.item_state_reward = item_state_reward\n",
        "        self.goal_state_reward = goal_state_reward\n",
        "\n",
        "        self.item = ItemObject() if item is None else item\n",
        "        self.agent = AgentObject()\n",
        "        self.fig = fig\n",
        "        self.ax = ax\n",
        "\n",
        "        if self.item.location is None:\n",
        "            self.item.set_location_randomly(self.n, self.n)\n",
        "\n",
        "        self.state: State\n",
        "        # TODO: possibly implmeent this if there are multiple GridObjects to check for\n",
        "        # initialize grid and put grid objects on the grid\n",
        "        # x_agent, y_agent = self.agent.location\n",
        "        # x_item, y_item = self.item.location\n",
        "        # self.grid = np.zeros((self.n, self.n))\n",
        "        # self.grid[x_agent, y_agent] = self.agent\n",
        "        # self.grid[x_item, y_item] = self.item\n",
        "\n",
        "        # Setup for animation\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None) -> None:\n",
        "        if agent_location is None:\n",
        "            self.agent.set_location_randomly(self.n, self.n, [self.item.get_location()])\n",
        "        else:\n",
        "            self.agent.location = agent_location\n",
        "        self.agent.has_item = False\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "        self.animate()  # Initial drawing of the grid\n",
        "\n",
        "    def get_state(self) -> State:\n",
        "        return self.state\n",
        "\n",
        "    def set_with_animation(self, with_animation: bool) -> None:\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def get_available_actions(self) -> list[Action]:\n",
        "        \"\"\"\n",
        "        Assumes that the current state is not the goal state\n",
        "        \"\"\"\n",
        "        # logic to determine available actions\n",
        "        actions = []\n",
        "        current_state = self.get_state()\n",
        "        x, y = current_state.agent_location\n",
        "\n",
        "        if current_state.agent_location == current_state.item_location and not current_state.has_item:\n",
        "            actions.append(Action.COLLECT)\n",
        "\n",
        "        # note: technically speaking we know that whenever agent is at the item location, the only available (or, the most optimal) action is to collect the item\n",
        "        # however, according to the CE, we must ensure that\n",
        "        # \"the agent is supposed to learn (rather than being told) that\n",
        "        # once it has picked up the load it needs to move to the delivery point to complete its mission. \",\n",
        "        # implyging that agent must be able to learn to \"collect\" instead of being told to collect (so add all possible actions)\n",
        "        if x > 0:\n",
        "            actions.append(Action.LEFT)  # left\n",
        "        if x < self.n - 1:\n",
        "            actions.append(Action.RIGHT)  # right\n",
        "        if y > 0:\n",
        "            actions.append(Action.DOWN)  # down\n",
        "        if y < self.n - 1:\n",
        "            actions.append(Action.UP)  # up\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def get_reward(self, prev_state: State, current_state: State):\n",
        "        \"\"\"\n",
        "        We can actually use self.state but to make it more explicit, we pass the states as an argument\n",
        "        \"\"\"\n",
        "        # TODO: technically, i think it should accept (prev state, action, next state)\n",
        "\n",
        "        # we ensure that Agent reveives item collection reward iff it has collected the item and is at the item location\n",
        "        # or else, in the item collected space, agent receives high reward by going back to where the item was (which is already collected so wrong)\n",
        "        if (\n",
        "            prev_state.agent_location == current_state.item_location\n",
        "            and current_state.agent_location == current_state.item_location\n",
        "            and current_state.has_item\n",
        "        ):\n",
        "            return self.item_state_reward\n",
        "        elif self.is_goal_state(current_state):\n",
        "            return self.goal_state_reward\n",
        "        else:\n",
        "            return self.time_penalty\n",
        "\n",
        "    def update_state(self, action: Action) -> None:\n",
        "        \"\"\"\n",
        "        Be careful: this method updates the state of the environment\n",
        "        \"\"\"\n",
        "        self.agent.move(action)\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "\n",
        "    def is_goal_state(self, state: State) -> bool:\n",
        "        return self.state.has_item and self.goal_location == state.agent_location\n",
        "\n",
        "    def animate(self):\n",
        "        if not self.with_animation:\n",
        "            return\n",
        "        self.ax.clear()\n",
        "        self.ax.set_xlim(0, self.n)\n",
        "        self.ax.set_ylim(0, self.n)\n",
        "        self.ax.set_xticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.set_yticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.grid(True)\n",
        "\n",
        "        # Plotting the agent, item, and goal\n",
        "        self.ax.text(\n",
        "            self.agent.location[0] + 0.5,\n",
        "            self.agent.location[1] + 0.5,\n",
        "            \"A\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"blue\" if not self.agent.has_item else \"purple\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.item.location[0] + 0.5,\n",
        "            self.item.location[1] + 0.5,\n",
        "            \"I\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"green\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.goal_location[0] + 0.5,\n",
        "            self.goal_location[1] + 0.5,\n",
        "            \"G\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"red\",\n",
        "        )\n",
        "\n",
        "        # TODO: add a message saying \"item collected\" if the agent has collected the item\n",
        "        # or else there is a single frame where the agent is at the same location twice,\n",
        "        # so it looks like the agent is not moving\n",
        "        handles = [\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"blue\", markersize=8, label=\"Agent (A)\")\n",
        "            if not self.agent.has_item\n",
        "            else plt.Line2D(\n",
        "                [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"purple\", markersize=8, label=\"Agent (A) with item\"\n",
        "            ),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"green\", markersize=8, label=\"Item (I)\"),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"red\", markersize=8, label=\"Goal (G)\"),\n",
        "        ]\n",
        "        self.ax.legend(handles=handles, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "        plt.subplots_adjust(right=0.75, left=0.1)\n",
        "        self.fig.canvas.draw_idle()\n",
        "        plt.pause(0.5)  # Pause to allow visualization of the movement\n",
        "\n",
        "    def step(self, action: Action) -> tuple[float, State]:\n",
        "        prev_state = self.get_state()\n",
        "        self.update_state(action)\n",
        "        next_state = self.get_state()\n",
        "        self.animate()\n",
        "        reward = self.get_reward(prev_state, next_state)\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "class GridObject(ABC):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        self.icon: str\n",
        "        self.location = (\n",
        "            location  # NOTE: location is a tuple of (x, y) where x and y are coordinates on the grid (not indices)\n",
        "        )\n",
        "\n",
        "    def set_location_randomly(\n",
        "        self, max_x: int, max_y: int, disallowed_locations: list[tuple[int, int]] = []\n",
        "    ) -> tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Note: max_x and max_y are exclusive\n",
        "\n",
        "        disallowed_locations: list of locations that are not allowed to be placed\n",
        "        (e.g. agent and item location should not be initialized to the same place)\n",
        "        \"\"\"\n",
        "        # The start, item, goal location must be different position\n",
        "        location = None\n",
        "        while location is None or location in disallowed_locations:\n",
        "            location = (randint(0, max_x - 1), randint(0, max_y - 1))\n",
        "\n",
        "        self.location = location\n",
        "        return location\n",
        "\n",
        "    def get_location(self) -> tuple[int, int]:\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Location is not set\")\n",
        "        return self.location\n",
        "\n",
        "\n",
        "class AgentObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        super().__init__(location)\n",
        "        self.icon = \"A\"\n",
        "        self.has_item = False  # TODO: has_item of AgentObject and State must be synched somehow\n",
        "\n",
        "    def move(self, action: Action) -> None:\n",
        "        # NOTE: assumes that action is valid (i.e. agent is not at the edge of the grid)\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Agent location is not set\")\n",
        "\n",
        "        x, y = self.location\n",
        "        if action == Action.LEFT:\n",
        "            self.location = (x - 1, y)  # left\n",
        "        elif action == Action.RIGHT:\n",
        "            self.location = (x + 1, y)  # right\n",
        "        elif action == Action.DOWN:\n",
        "            self.location = (x, y - 1)  # down\n",
        "        elif action == Action.UP:\n",
        "            self.location = (x, y + 1)  # up\n",
        "        elif action == Action.COLLECT:\n",
        "            self.has_item = True\n",
        "\n",
        "\n",
        "class ItemObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None):\n",
        "        super().__init__(location)\n",
        "        self.icon = \"I\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rXbaIuD_cf7"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class State:\n",
        "    # it doesn not hold AgentObject / ItemObject because I want State to be immutable\n",
        "    # but in the future, we might want to add more attributes to State\n",
        "    # in that case we need to make a copy of the AgentObject / ItemObject\n",
        "    agent_location: tuple[int, int]\n",
        "    item_location: tuple[int, int]\n",
        "\n",
        "    has_item: bool = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91lnffe6_cf7"
      },
      "source": [
        "**<p>Learning Process:</p>**\n",
        "In tabular Q-learning, an agent will learn to make optimal decisions by estimating the expected rewards for state-action pairs through iteratively updating Q-values. Our training process are following:\n",
        "\n",
        "<ol><li>Initialize the Q-value matrix for the dimension explained above, (agent_location_x, agent_location_y, is_item_collected).</li>\n",
        "<li>Multiple environments with different item positions of  (item_location_x, item_location_y) are initialized.</li>\n",
        "<li>For each environment, we train the Q Value Matrix, using hyper-parameters such as learning rate, discount factor, epsilon (exploration rate), and the number of episodes. The training is managed by the Trainer class while updating Q Value and choosing the best action is handled by the Agent class.</li>\n",
        "<li>With the epsilon-greedy policy, the agent will occasionally take random actions to explore new possibilities while mostly choosing actions with the highest estimated rewards while exploring the environment.</li>\n",
        "<li>Following the Q-learning update rule, the agent updates its Q-values based on the rewards received and the estimated future rewards.</li>\n",
        "<li>Once a state is explored, the agent will move to the next stage and repeat until the goal is reached.</li>\n",
        "<li>Steps 3 to 6 are repeated for a predetermined number of episodes until the maximum number of episodes is reached.</li>\n",
        "<li>After training, the agent's performance is evaluated by testing its behaviour in environments and visualising its path to understand its decision-making.</li>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6k-yDKg_cf8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "class QValueMatrix:\n",
        "    \"\"\"\n",
        "    Abstracts the Q-value matrix for the agent,\n",
        "    to hide different q value matrices for different states and action handling\n",
        "    \"\"\"\n",
        "    def __init__(self, x_max: int, y_max: int, num_max_actions: int) -> None:\n",
        "        # TODO: check item_location and goal_location are within the grid\n",
        "        # TODO: the way we are stroing the q values is memory inefficient in a way that\n",
        "        # not all state will have all actions (we are storing 0 for those)\n",
        "\n",
        "        self.start_to_item = np.zeros((x_max, y_max, num_max_actions))\n",
        "        self.item_to_goal = np.zeros((x_max, y_max, num_max_actions))\n",
        "\n",
        "    def get_state_qvals(self, state: State, actions: list[Action] | Action = []) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns Q(S), or Q(S, A) if actions are provided\n",
        "        \"\"\"\n",
        "        if isinstance(actions, Action):\n",
        "            actions = [actions]\n",
        "\n",
        "        x, y = state.agent_location\n",
        "        if state.has_item:\n",
        "            return self.item_to_goal[x, y] if not actions else self.item_to_goal[x, y, [action.value for action in actions]]\n",
        "        else:\n",
        "            return self.start_to_item[x, y] if not actions else self.start_to_item[x, y, [action.value for action in actions]]\n",
        "\n",
        "    def update_qval(self, state: State, action: Action, new_qval: float) -> None:\n",
        "        \"\"\"\n",
        "        Updates the Q value for a state-action pair, i.e. Q(S, A) = new_qval\n",
        "        \"\"\"\n",
        "        x, y = state.agent_location\n",
        "        if state.has_item:\n",
        "            self.item_to_goal[x, y, action.value] = new_qval\n",
        "        else:\n",
        "            self.start_to_item[x, y, action.value] = new_qval\n",
        "\n",
        "    def increase_qval(self, state: State, action: Action, increment: float) -> None:\n",
        "        \"\"\"\n",
        "        Increases the Q value for a state-action pair, i.e. Q(S, A) += increment\n",
        "        \"\"\"\n",
        "        x, y = state.agent_location\n",
        "        if state.has_item:\n",
        "            self.item_to_goal[x, y, action.value] += increment\n",
        "        else:\n",
        "            self.start_to_item[x, y, action.value] += increment\n",
        "\n",
        "\n",
        "def generate_grid_location_list(max_x: int, max_y) -> list[tuple[int, int]]:\n",
        "    return [(i, j) for i in range(max_x) for j in range(max_y)]\n",
        "\n",
        "\n",
        "def save_trained_qval_matrix(trained_qval_matrix: QValueMatrix, item: ItemObject) -> None:\n",
        "    if item.location is None:\n",
        "        raise ValueError(\"Item location is None\")\n",
        "    with open(f'qval_matrix{item.location[0]}_{item.location[1]}.pickle', \"wb\") as f:\n",
        "        pickle.dump(trained_qval_matrix, f)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha: float = 0.3,\n",
        "        discount_rate: float = 0.9,\n",
        "        epsilon: float = 0.1,\n",
        "        num_episode_per_intermediate_item: int = 1000,\n",
        "        grid_size: tuple[int, int] = (5, 5),\n",
        "        save_weights: bool = False,\n",
        "    ) -> None:\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.discount_rate = discount_rate\n",
        "        self.num_episode_per_intermediate_item = num_episode_per_intermediate_item\n",
        "        self.grid_size = grid_size\n",
        "        self.save_weights = save_weights\n",
        "\n",
        "        self.trained_qval_matrices: list[QValueMatrix] = []\n",
        "\n",
        "    def update(self, current_state: State, next_state: State, reward: float, action: Action, qval_matrix: QValueMatrix) -> None:\n",
        "        qval_difference: float = self.alpha * (\n",
        "            reward\n",
        "            + self.discount_rate * np.max(qval_matrix.get_state_qvals(next_state))\n",
        "            - qval_matrix.get_state_qvals(current_state, actions=action)\n",
        "        )\n",
        "        qval_matrix.increase_qval(current_state, action, qval_difference)\n",
        "\n",
        "    def choose_action(self, possible_actions: list[Action], state: State, qval_matrix: QValueMatrix, is_training: bool = True) -> Action:\n",
        "        \"\"\"\n",
        "        Epislon greedy method to choose action\n",
        "        \"\"\"\n",
        "        if is_training and random.random() < self.epsilon:\n",
        "            return random.choice(possible_actions)\n",
        "        else:\n",
        "            action_to_qval = list(zip(possible_actions, qval_matrix.get_state_qvals(state, actions=possible_actions)))\n",
        "            random.shuffle(action_to_qval)  # to break ties randomly\n",
        "            return max(action_to_qval, key=lambda x: x[1])[0]\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, agent: Agent, envs: list[Environment]) -> None:\n",
        "        self.agent = agent\n",
        "        self.environments = envs\n",
        "\n",
        "    def train_one(self, num_episodes: int, env: Environment) -> QValueMatrix:\n",
        "        \"\"\"\n",
        "        Conducts training for a given number of episodes.\n",
        "        \"\"\"\n",
        "        qval_matrix = QValueMatrix(self.agent.grid_size[0], self.agent.grid_size[1], len(Action))\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            env.initialize_for_new_episode()\n",
        "            current_state = env.get_state()\n",
        "            while not env.is_goal_state(current_state):\n",
        "                possible_actions = env.get_available_actions()\n",
        "                action = self.agent.choose_action(possible_actions, current_state, qval_matrix)\n",
        "                reward, next_state = env.step(action)\n",
        "                self.agent.update(current_state, next_state, reward, action, qval_matrix)\n",
        "                current_state = next_state\n",
        "\n",
        "        return qval_matrix\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        We are training for all \"goal location\" in the grid; so indivisual state consists of x, y, goal_x, goal_y, technically speaking.\n",
        "        However, to ensure that the agent samples from all possible goal locations fairly, we will separately train for all possible goal locations.\n",
        "        \"\"\"\n",
        "\n",
        "        for env in self.environments:\n",
        "            qval_matrix = self.train_one(self.agent.num_episode_per_intermediate_item, env)\n",
        "\n",
        "            # Store the trained Q-value matrix in the agent\n",
        "            self.agent.trained_qval_matrices.append(qval_matrix)\n",
        "\n",
        "            if self.agent.save_weights:\n",
        "                save_trained_qval_matrix(qval_matrix, env.item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRB_7URP_cf9"
      },
      "source": [
        "### Metric:\n",
        "$$ \\frac{1}{number\\ of\\ episodes}\\sum_{item\\ location}\\sum_{agent\\ location}{\\frac{M(agent\\ location, item\\ location) + 1 + M(item\\ location, goal\\ location)}{number\\ of\\ steps\\ taken}} $$\n",
        "where M represents Manhattan distance\n",
        "\n",
        "  We began by training our Q-learner for all possible item locations and a fixed goal position. After training, we evaluated the agent's performance by calculating the ratio of the shortest possible distance (using Manhattan distance) to the actual number of steps taken by the agent from the start position, through the item, and finally reaching the goal (shortest distance / actual distance taken by the agent). This calculation was done for all possible item and agent locations within the environment. A ratio of 1 indicates that the actual path matches the shortest path, while a ratio less than 1 suggests a longer path was taken. By averaging these ratios across all possible scenarios, we obtained an average value greater than 0.975, indicating that our Q-learner effectively learns the optimal paths in the environment. Note that 1 in the metric represents additional step to pick up item.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynJXxYaN_cf9",
        "outputId": "e5427acd-32a4-4e78-a32f-561273437cd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hf/185pxfjs633fp7m0867r7t6m0000gn/T/ipykernel_2814/1550507816.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.start_to_item[x, y, action.value] += increment\n",
            "/var/folders/hf/185pxfjs633fp7m0867r7t6m0000gn/T/ipykernel_2814/1550507816.py:47: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  self.item_to_goal[x, y, action.value] += increment\n",
            "100%|██████████| 25/25 [00:00<00:00, 17909.07it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 21098.11it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 18883.05it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 18013.67it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 15168.18it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 22790.18it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 22187.39it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 22584.02it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 23989.38it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 16391.68it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 26879.67it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 27094.99it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 31479.32it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 24510.89it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 21030.41it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 32635.42it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 31794.30it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 26419.15it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 24800.76it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 23600.63it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 31488.77it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 34583.64it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 28248.28it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 24824.24it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 16972.74it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 260.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average performance score (1 is the best): 0.9699\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, n=5) -> None:\n",
        "        self.n = n\n",
        "        self.agent = Agent(num_episode_per_intermediate_item=1000)\n",
        "        item_grid_locations = generate_grid_location_list(self.n, self.n)\n",
        "        all_items = [ItemObject(grid_location) for grid_location in item_grid_locations]\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        self.envs = [Environment(item = item, with_animation=False, fig = fig, ax = ax) for item in all_items]\n",
        "\n",
        "    def run_train(self) -> None:\n",
        "        \"\"\"\n",
        "        Trains the agent in the environment and returns the trained agent.\n",
        "        \"\"\"\n",
        "        trainer = Trainer(self.agent, self.envs)\n",
        "        trainer.train()\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_manhattan_distance(start_location: tuple[int, int], goal_location: tuple[int, int]) -> int:\n",
        "        \"\"\"\n",
        "        Calculates the Manhattan distance between two points.\n",
        "        \"\"\"\n",
        "        start_x, start_y = start_location\n",
        "        goal_x, goal_y = goal_location\n",
        "        return abs(start_x - goal_x) + abs(start_y - goal_y)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics_score(shortest_distance: int, distance: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the proportion of the Q-learning distance to the shortest distance.\n",
        "        \"\"\"\n",
        "        return shortest_distance / distance\n",
        "\n",
        "    def visualize(self, num_of_vis: int = 5) -> None:\n",
        "        \"\"\"\n",
        "        Visualize the path after trained\n",
        "        \"\"\"\n",
        "        env_indices = random.sample(range(1, self.n*self.n), num_of_vis)\n",
        "        for index in env_indices:\n",
        "            env = self.envs[index]\n",
        "            env.set_with_animation(True)\n",
        "            env.initialize_for_new_episode()\n",
        "\n",
        "            # Run the agent in the environment\n",
        "            current_state = env.get_state()\n",
        "            while not env.is_goal_state(current_state):\n",
        "                possible_actions = env.get_available_actions()\n",
        "                action = self.agent.choose_action(possible_actions, current_state, self.agent.trained_qval_matrices[index], is_training=False)\n",
        "                _, next_state = env.step(action)\n",
        "                current_state = next_state\n",
        "\n",
        "    def performance_test(self):\n",
        "        \"\"\"\n",
        "        Conducts a performance test ()\n",
        "        \"\"\"\n",
        "        num_episodes = 0\n",
        "        total_score = 0\n",
        "        for env in tqdm(self.envs):\n",
        "            env.set_with_animation(False)\n",
        "            for agent_locaiton in tqdm(generate_grid_location_list(self.n, self.n)):\n",
        "                if agent_locaiton == env.item.location or agent_locaiton == env.goal_location:\n",
        "                    continue\n",
        "\n",
        "                env.initialize_for_new_episode(agent_location=agent_locaiton)\n",
        "                start_location = env.agent.location  # Get the start location of the agent\n",
        "                item_location = env.item.location  # Get intermediate location of the item\n",
        "\n",
        "                # Calculate shortest distance from start to item to goal\n",
        "                shortest_distance = (\n",
        "                    self.calculate_manhattan_distance(start_location, item_location)\n",
        "                    + 1\n",
        "                    + self.calculate_manhattan_distance(item_location, env.goal_location)\n",
        "                )\n",
        "\n",
        "                current_state = env.get_state()\n",
        "                num_steps = 0\n",
        "                while not env.is_goal_state(current_state):\n",
        "                    possible_actions = env.get_available_actions()\n",
        "                    item_x, item_y = current_state.item_location\n",
        "                    action = self.agent.choose_action(possible_actions, current_state, self.agent.trained_qval_matrices[self.n*item_x+item_y], is_training=False)\n",
        "                    _, next_state = env.step(action)\n",
        "                    current_state = next_state\n",
        "                    num_steps += 1\n",
        "\n",
        "                # Calculate and accumulate the score\n",
        "                total_score += self.calculate_metrics_score(shortest_distance, num_steps)\n",
        "\n",
        "                num_episodes += 1\n",
        "\n",
        "        # Return the average score across all tests\n",
        "        return total_score / num_episodes\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evl = Evaluation()\n",
        "    evl.run_train()\n",
        "\n",
        "    # Conduct the performance test\n",
        "    average_score = evl.performance_test()\n",
        "    print(f\"Average performance score (1 is the best): {average_score:.4f}\")\n",
        "\n",
        "    # visualize randomly the environments and show the steps of the agent\n",
        "    evl.visualize()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}