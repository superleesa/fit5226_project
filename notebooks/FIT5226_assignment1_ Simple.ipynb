{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    # NOTE: QValue matrix used these action values as their indices\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    UP = 3\n",
    "    \n",
    "    # actions when agent just got the item and is moving to item_reached state\n",
    "    COLLECT = 4  # goes to item reached state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from abc import ABC\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.axis\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEFAULT_TIME_PENALTY = -1\n",
    "GOAL_STATE_REWARD = 300\n",
    "DEFAULT_ITEM_REWARD = 200\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n: int = 5,\n",
    "        fig: matplotlib.figure.Figure | None = None,\n",
    "        ax: matplotlib.axes.Axes | None = None,\n",
    "        item: ItemObject | None = None,\n",
    "        goal_location: tuple[int, int] = (4, 0),\n",
    "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
    "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
    "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
    "        with_animation: bool = True,\n",
    "    ) -> None:\n",
    "        self.n = n\n",
    "        self.goal_location = goal_location\n",
    "        self.time_penalty = time_penalty\n",
    "        self.item_state_reward = item_state_reward\n",
    "        self.goal_state_reward = goal_state_reward\n",
    "\n",
    "        self.item = ItemObject() if item is None else item\n",
    "        self.agent = AgentObject()\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "\n",
    "        if self.item.location is None:\n",
    "            self.item.set_location_randomly(self.n, self.n)\n",
    "\n",
    "        self.state: State\n",
    "        # TODO: possibly implmeent this if there are multiple GridObjects to check for\n",
    "        # initialize grid and put grid objects on the grid\n",
    "        # x_agent, y_agent = self.agent.location\n",
    "        # x_item, y_item = self.item.location\n",
    "        # self.grid = np.zeros((self.n, self.n))\n",
    "        # self.grid[x_agent, y_agent] = self.agent\n",
    "        # self.grid[x_item, y_item] = self.item\n",
    "\n",
    "        # Setup for animation\n",
    "        self.with_animation = with_animation\n",
    "\n",
    "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None) -> None:\n",
    "        if agent_location is None:\n",
    "            self.agent.set_location_randomly(self.n, self.n, [self.item.get_location()]) \n",
    "        else:\n",
    "            self.agent.location = agent_location\n",
    "        self.agent.has_item = False\n",
    "        self.state = State(\n",
    "            agent_location=self.agent.get_location(),\n",
    "            item_location=self.item.get_location(),\n",
    "            has_item=self.agent.has_item,\n",
    "        )\n",
    "        self.animate()  # Initial drawing of the grid\n",
    "\n",
    "    def get_state(self) -> State:\n",
    "        return self.state\n",
    "    \n",
    "    def set_with_animation(self, with_animation: bool) -> None:\n",
    "        self.with_animation = with_animation\n",
    "\n",
    "    def get_available_actions(self) -> list[Action]:\n",
    "        \"\"\"\n",
    "        Assumes that the current state is not the goal state\n",
    "        \"\"\"\n",
    "        # logic to determine available actions\n",
    "        actions = []\n",
    "        current_state = self.get_state()\n",
    "        x, y = current_state.agent_location\n",
    "\n",
    "        if current_state.agent_location == current_state.item_location and not current_state.has_item:\n",
    "            actions.append(Action.COLLECT)\n",
    "\n",
    "        # note: technically speaking we know that whenever agent is at the item location, the only available (or, the most optimal) action is to collect the item\n",
    "        # however, according to the CE, we must ensure that\n",
    "        # \"the agent is supposed to learn (rather than being told) that\n",
    "        # once it has picked up the load it needs to move to the delivery point to complete its mission. \",\n",
    "        # implyging that agent must be able to learn to \"collect\" instead of being told to collect (so add all possible actions)\n",
    "        if x > 0:\n",
    "            actions.append(Action.LEFT)  # left\n",
    "        if x < self.n - 1:\n",
    "            actions.append(Action.RIGHT)  # right\n",
    "        if y > 0:\n",
    "            actions.append(Action.DOWN)  # down\n",
    "        if y < self.n - 1:\n",
    "            actions.append(Action.UP)  # up\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_reward(self, prev_state: State, current_state: State):\n",
    "        \"\"\"\n",
    "        We can actually use self.state but to make it more explicit, we pass the states as an argument\n",
    "        \"\"\"\n",
    "        # TODO: technically, i think it should accept (prev state, action, next state)\n",
    "\n",
    "        # we ensure that Agent reveives item collection reward iff it has collected the item and is at the item location\n",
    "        # or else, in the item collected space, agent receives high reward by going back to where the item was (which is already collected so wrong)\n",
    "        if (\n",
    "            prev_state.agent_location == current_state.item_location\n",
    "            and current_state.agent_location == current_state.item_location\n",
    "            and current_state.has_item\n",
    "        ):\n",
    "            return self.item_state_reward\n",
    "        elif self.is_goal_state(current_state):\n",
    "            return self.goal_state_reward\n",
    "        else:\n",
    "            return self.time_penalty\n",
    "\n",
    "    def update_state(self, action: Action) -> None:\n",
    "        \"\"\"\n",
    "        Be careful: this method updates the state of the environment\n",
    "        \"\"\"\n",
    "        self.agent.move(action)\n",
    "        self.state = State(\n",
    "            agent_location=self.agent.get_location(),\n",
    "            item_location=self.item.get_location(),\n",
    "            has_item=self.agent.has_item,\n",
    "        )\n",
    "\n",
    "    def is_goal_state(self, state: State) -> bool:\n",
    "        return self.state.has_item and self.goal_location == state.agent_location\n",
    "\n",
    "    def animate(self):\n",
    "        if not self.with_animation:\n",
    "            return\n",
    "        self.ax.clear()\n",
    "        self.ax.set_xlim(0, self.n)\n",
    "        self.ax.set_ylim(0, self.n)\n",
    "        self.ax.set_xticks(np.arange(0, self.n + 1, 1))\n",
    "        self.ax.set_yticks(np.arange(0, self.n + 1, 1))\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        # Plotting the agent, item, and goal\n",
    "        self.ax.text(\n",
    "            self.agent.location[0] + 0.5,\n",
    "            self.agent.location[1] + 0.5,\n",
    "            \"A\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=16,\n",
    "            color=\"blue\" if not self.agent.has_item else \"purple\",\n",
    "        )\n",
    "        self.ax.text(\n",
    "            self.item.location[0] + 0.5,\n",
    "            self.item.location[1] + 0.5,\n",
    "            \"I\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=16,\n",
    "            color=\"green\",\n",
    "        )\n",
    "        self.ax.text(\n",
    "            self.goal_location[0] + 0.5,\n",
    "            self.goal_location[1] + 0.5,\n",
    "            \"G\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=16,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "        # TODO: add a message saying \"item collected\" if the agent has collected the item\n",
    "        # or else there is a single frame where the agent is at the same location twice,\n",
    "        # so it looks like the agent is not moving\n",
    "        handles = [\n",
    "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"blue\", markersize=8, label=\"Agent (A)\")\n",
    "            if not self.agent.has_item\n",
    "            else plt.Line2D(\n",
    "                [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"purple\", markersize=8, label=\"Agent (A) with item\"\n",
    "            ),\n",
    "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"green\", markersize=8, label=\"Item (I)\"),\n",
    "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"red\", markersize=8, label=\"Goal (G)\"),\n",
    "        ]\n",
    "        self.ax.legend(handles=handles, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "        plt.subplots_adjust(right=0.75, left=0.1)\n",
    "        self.fig.canvas.draw_idle()\n",
    "        plt.pause(0.5)  # Pause to allow visualization of the movement\n",
    "\n",
    "    def step(self, action: Action) -> tuple[float, State]:\n",
    "        prev_state = self.get_state()\n",
    "        self.update_state(action)\n",
    "        next_state = self.get_state()\n",
    "        self.animate()\n",
    "        reward = self.get_reward(prev_state, next_state)\n",
    "        return reward, next_state\n",
    "\n",
    "\n",
    "class GridObject(ABC):\n",
    "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
    "        self.icon: str\n",
    "        self.location = (\n",
    "            location  # NOTE: location is a tuple of (x, y) where x and y are coordinates on the grid (not indices)\n",
    "        )\n",
    "\n",
    "    def set_location_randomly(\n",
    "        self, max_x: int, max_y: int, disallowed_locations: list[tuple[int, int]] = []\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Note: max_x and max_y are exclusive\n",
    "\n",
    "        disallowed_locations: list of locations that are not allowed to be placed\n",
    "        (e.g. agent and item location should not be initialized to the same place)\n",
    "        \"\"\"\n",
    "        # The start, item, goal location must be different position\n",
    "        location = None\n",
    "        while location is None or location in disallowed_locations:\n",
    "            location = (randint(0, max_x - 1), randint(0, max_y - 1))\n",
    "\n",
    "        self.location = location\n",
    "        return location\n",
    "\n",
    "    def get_location(self) -> tuple[int, int]:\n",
    "        if self.location is None:\n",
    "            raise ValueError(\"Location is not set\")\n",
    "        return self.location\n",
    "\n",
    "\n",
    "class AgentObject(GridObject):\n",
    "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
    "        super().__init__(location)\n",
    "        self.icon = \"A\"\n",
    "        self.has_item = False  # TODO: has_item of AgentObject and State must be synched somehow\n",
    "\n",
    "    def move(self, action: Action) -> None:\n",
    "        # NOTE: assumes that action is valid (i.e. agent is not at the edge of the grid)\n",
    "        if self.location is None:\n",
    "            raise ValueError(\"Agent location is not set\")\n",
    "\n",
    "        x, y = self.location\n",
    "        if action == Action.LEFT:\n",
    "            self.location = (x - 1, y)  # left\n",
    "        elif action == Action.RIGHT:\n",
    "            self.location = (x + 1, y)  # right\n",
    "        elif action == Action.DOWN:\n",
    "            self.location = (x, y - 1)  # down\n",
    "        elif action == Action.UP:\n",
    "            self.location = (x, y + 1)  # up\n",
    "        elif action == Action.COLLECT:\n",
    "            self.has_item = True\n",
    "\n",
    "\n",
    "class ItemObject(GridObject):\n",
    "    def __init__(self, location: tuple[int, int] | None = None):\n",
    "        super().__init__(location)\n",
    "        self.icon = \"I\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    # it doesn not hold AgentObject / ItemObject because I want State to be immutable\n",
    "    # but in the future, we might want to add more attributes to State\n",
    "    # in that case we need to make a copy of the AgentObject / ItemObject\n",
    "    agent_location: tuple[int, int]\n",
    "    item_location: tuple[int, int]\n",
    "    \n",
    "    has_item: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "class QValueMatrix:\n",
    "    \"\"\"\n",
    "    Abstracts the Q-value matrix for the agent,\n",
    "    to hide different q value matrices for different states and action handling\n",
    "    \"\"\"\n",
    "    def __init__(self, x_max: int, y_max: int, num_max_actions: int) -> None:\n",
    "        # TODO: check item_location and goal_location are within the grid\n",
    "        # TODO: the way we are stroing the q values is memory inefficient in a way that\n",
    "        # not all state will have all actions (we are storing 0 for those)\n",
    "        \n",
    "        self.start_to_item = np.zeros((x_max, y_max, num_max_actions))\n",
    "        self.item_to_goal = np.zeros((x_max, y_max, num_max_actions))\n",
    "    \n",
    "    def get_state_qvals(self, state: State, actions: list[Action] | Action = []) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns Q(S), or Q(S, A) if actions are provided\n",
    "        \"\"\"\n",
    "        if isinstance(actions, Action):\n",
    "            actions = [actions]\n",
    "        \n",
    "        x, y = state.agent_location\n",
    "        if state.has_item:\n",
    "            return self.item_to_goal[x, y] if not actions else self.item_to_goal[x, y, [action.value for action in actions]]\n",
    "        else:\n",
    "            return self.start_to_item[x, y] if not actions else self.start_to_item[x, y, [action.value for action in actions]]\n",
    "    \n",
    "    def update_qval(self, state: State, action: Action, new_qval: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q value for a state-action pair, i.e. Q(S, A) = new_qval\n",
    "        \"\"\"\n",
    "        x, y = state.agent_location\n",
    "        if state.has_item:\n",
    "            self.item_to_goal[x, y, action.value] = new_qval\n",
    "        else:\n",
    "            self.start_to_item[x, y, action.value] = new_qval\n",
    "    \n",
    "    def increase_qval(self, state: State, action: Action, increment: float) -> None:\n",
    "        \"\"\"\n",
    "        Increases the Q value for a state-action pair, i.e. Q(S, A) += increment\n",
    "        \"\"\"\n",
    "        x, y = state.agent_location\n",
    "        if state.has_item:\n",
    "            self.item_to_goal[x, y, action.value] += increment\n",
    "        else:\n",
    "            self.start_to_item[x, y, action.value] += increment\n",
    "\n",
    "\n",
    "def generate_grid_location_list(max_x: int, max_y) -> list[tuple[int, int]]:\n",
    "    return [(i, j) for i in range(max_x) for j in range(max_y)]\n",
    "\n",
    "\n",
    "def save_trained_qval_matrix(trained_qval_matrix: QValueMatrix, item: ItemObject) -> None:\n",
    "    if item.location is None:\n",
    "        raise ValueError(\"Item location is None\")\n",
    "    with open(f'qval_matrix{item.location[0]}_{item.location[1]}.pickle', \"wb\") as f:\n",
    "        pickle.dump(trained_qval_matrix, f)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.3,\n",
    "        discount_rate: float = 0.9,\n",
    "        epsilon: float = 0.1,\n",
    "        num_episode_per_intermediate_item: int = 1000,\n",
    "        grid_size: tuple[int, int] = (5, 5),\n",
    "        save_weights: bool = False,\n",
    "    ) -> None:\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.num_episode_per_intermediate_item = num_episode_per_intermediate_item\n",
    "        self.grid_size = grid_size\n",
    "        self.save_weights = save_weights\n",
    "        \n",
    "        self.trained_qval_matrices: list[QValueMatrix] = []\n",
    "    \n",
    "    def update(self, current_state: State, next_state: State, reward: float, action: Action, qval_matrix: QValueMatrix) -> None:\n",
    "        qval_difference: float = self.alpha * (\n",
    "            reward\n",
    "            + self.discount_rate * np.max(qval_matrix.get_state_qvals(next_state))\n",
    "            - qval_matrix.get_state_qvals(current_state, actions=action)\n",
    "        )\n",
    "        qval_matrix.increase_qval(current_state, action, qval_difference)\n",
    "\n",
    "    def choose_action(self, possible_actions: list[Action], state: State, qval_matrix: QValueMatrix, is_training: bool = True) -> Action:\n",
    "        \"\"\"\n",
    "        Epislon greedy method to choose action\n",
    "        \"\"\"\n",
    "        if is_training and random.random() < self.epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "        else:\n",
    "            action_to_qval = list(zip(possible_actions, qval_matrix.get_state_qvals(state, actions=possible_actions)))\n",
    "            random.shuffle(action_to_qval)  # to break ties randomly\n",
    "            return max(action_to_qval, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, agent: Agent, envs: list[Environment]) -> None:\n",
    "        self.agent = agent\n",
    "        self.environments = envs\n",
    "\n",
    "    def train_one(self, num_episodes: int, env: Environment) -> QValueMatrix:\n",
    "        \"\"\"\n",
    "        Conducts training for a given number of episodes.\n",
    "        \"\"\"\n",
    "        qval_matrix = QValueMatrix(self.agent.grid_size[0], self.agent.grid_size[1], len(Action))\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            env.initialize_for_new_episode()\n",
    "            current_state = env.get_state()\n",
    "            while not env.is_goal_state(current_state):\n",
    "                possible_actions = env.get_available_actions()\n",
    "                action = self.agent.choose_action(possible_actions, current_state, qval_matrix)\n",
    "                reward, next_state = env.step(action)\n",
    "                self.agent.update(current_state, next_state, reward, action, qval_matrix)\n",
    "                current_state = next_state\n",
    "        \n",
    "        return qval_matrix\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        We are training for all \"goal location\" in the grid; so indivisual state consists of x, y, goal_x, goal_y, technically speaking.\n",
    "        However, to ensure that the agent samples from all possible goal locations fairly, we will separately train for all possible goal locations.\n",
    "        \"\"\"\n",
    "        \n",
    "        for env in self.environments:\n",
    "            qval_matrix = self.train_one(self.agent.num_episode_per_intermediate_item, env)\n",
    "            \n",
    "            # Store the trained Q-value matrix in the agent\n",
    "            self.agent.trained_qval_matrices.append(qval_matrix)\n",
    "            \n",
    "            if self.agent.save_weights:\n",
    "                save_trained_qval_matrix(qval_matrix, env.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/185pxfjs633fp7m0867r7t6m0000gn/T/ipykernel_56485/1550507816.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.start_to_item[x, y, action.value] += increment\n",
      "/var/folders/hf/185pxfjs633fp7m0867r7t6m0000gn/T/ipykernel_56485/1550507816.py:47: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.item_to_goal[x, y, action.value] += increment\n",
      "100%|██████████| 25/25 [00:00<00:00, 19717.49it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 20800.95it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 17766.45it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 14696.23it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 16745.07it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 21460.83it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 27740.11it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 25327.92it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 20867.18it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 12262.61it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 27349.40it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 27442.45it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 22584.02it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 19515.65it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 15952.78it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 24579.84it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 33737.97it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 33511.54it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 23337.99it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 20207.67it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 36934.70it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 25562.55it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 26313.07it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 21718.64it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 22711.20it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 251.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance score (1 is the best): 0.9721\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, n=5) -> None:\n",
    "        self.n = n\n",
    "        self.agent = Agent(num_episode_per_intermediate_item=1000)\n",
    "        item_grid_locations = generate_grid_location_list(self.n, self.n)\n",
    "        all_items = [ItemObject(grid_location) for grid_location in item_grid_locations]\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        self.envs = [Environment(item = item, with_animation=False, fig = fig, ax = ax) for item in all_items]\n",
    "    \n",
    "    def run_train(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains the agent in the environment and returns the trained agent.\n",
    "        \"\"\"\n",
    "        trainer = Trainer(self.agent, self.envs)\n",
    "        trainer.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_manhattan_distance(start_location: tuple[int, int], goal_location: tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the Manhattan distance between two points.\n",
    "        \"\"\"\n",
    "        start_x, start_y = start_location\n",
    "        goal_x, goal_y = goal_location\n",
    "        return abs(start_x - goal_x) + abs(start_y - goal_y)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics_score(shortest_distance: int, distance: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the proportion of the Q-learning distance to the shortest distance.\n",
    "        \"\"\"\n",
    "        return shortest_distance / distance\n",
    "\n",
    "    def visualize(self, num_of_vis: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the path after trained\n",
    "        \"\"\"\n",
    "        env_indices = random.sample(range(1, self.n*self.n), num_of_vis)\n",
    "        for index in env_indices:\n",
    "            env = self.envs[index]\n",
    "            env.set_with_animation(True)\n",
    "            env.initialize_for_new_episode()\n",
    "\n",
    "            # Run the agent in the environment\n",
    "            current_state = env.get_state()\n",
    "            while not env.is_goal_state(current_state):\n",
    "                possible_actions = env.get_available_actions()\n",
    "                action = self.agent.choose_action(possible_actions, current_state, self.agent.trained_qval_matrices[index], is_training=False)\n",
    "                _, next_state = env.step(action)\n",
    "                current_state = next_state\n",
    "    \n",
    "    def performance_test(self):\n",
    "        \"\"\"\n",
    "        Conducts a performance test ()\n",
    "        \"\"\"\n",
    "        num_episodes = 0\n",
    "        total_score = 0\n",
    "        for env in tqdm(self.envs):\n",
    "            env.set_with_animation(False)\n",
    "            for agent_locaiton in tqdm(generate_grid_location_list(self.n, self.n)):\n",
    "                if agent_locaiton == env.item.location or agent_locaiton == env.goal_location:\n",
    "                    continue\n",
    "\n",
    "                env.initialize_for_new_episode(agent_location=agent_locaiton)\n",
    "                start_location = env.agent.location  # Get the start location of the agent\n",
    "                item_location = env.item.location  # Get intermediate location of the item\n",
    "\n",
    "                # Calculate shortest distance from start to item to goal\n",
    "                shortest_distance = (\n",
    "                    self.calculate_manhattan_distance(start_location, item_location)\n",
    "                    + 1 \n",
    "                    + self.calculate_manhattan_distance(item_location, env.goal_location)\n",
    "                )\n",
    "\n",
    "                current_state = env.get_state()\n",
    "                num_steps = 0\n",
    "                while not env.is_goal_state(current_state):\n",
    "                    possible_actions = env.get_available_actions()\n",
    "                    item_x, item_y = current_state.item_location\n",
    "                    action = self.agent.choose_action(possible_actions, current_state, self.agent.trained_qval_matrices[self.n*item_x+item_y], is_training=False)\n",
    "                    _, next_state = env.step(action)\n",
    "                    current_state = next_state\n",
    "                    num_steps += 1\n",
    "\n",
    "                # Calculate and accumulate the score\n",
    "                total_score += self.calculate_metrics_score(shortest_distance, num_steps)\n",
    "                \n",
    "                num_episodes += 1\n",
    "\n",
    "        # Return the average score across all tests\n",
    "        return total_score / num_episodes\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    evl = Evaluation()\n",
    "    evl.run_train()\n",
    "\n",
    "    # Conduct the performance test\n",
    "    average_score = evl.performance_test()\n",
    "    print(f\"Average performance score (1 is the best): {average_score:.4f}\")\n",
    "\n",
    "    # visualize randomly the environments and show the steps of the agent\n",
    "    evl.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
