{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgSDrXY_5tv"
      },
      "source": [
        "# FIT5226 Assignment 2 - Deep Q Learning\n",
        "\n",
        "Team Name: Simple <br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Team Members: Satoshi Kashima, Shosuke Asano, Tanul Gupta, Felix Tay Shi Hong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (4.66.1)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "                                              0.0/362.8 kB ? eta -:--:--\n",
            "     -------------------------------------  358.4/362.8 kB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- 362.8/362.8 kB 7.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.24.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (3.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (1.13.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (23.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (2.0.34)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: Mako in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\super\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.8.2 optuna-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm optuna numpy scipy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TkAgg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptQ36NY__5nM"
      },
      "source": [
        "**<p>Action:</p>**\n",
        "\n",
        "We have 5 actions: going left, right, down, up, and collecting an item. The action other than collecting items is generally available to all states (except where itâ€™s inappropriate e.g. at the right-top corner, only available actions should be left and down actions). The action to collect the item is only available at the first time visiting the item location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5fZ5gJ4MKghh"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Action(Enum):\n",
        "    # NOTE: QValue matrix used these action values as their indices\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "    DOWN = 2\n",
        "    UP = 3\n",
        "\n",
        "    # actions when agent just got the item and is moving to item_reached state\n",
        "    COLLECT = 4  # goes to item reached state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmfE4waBXyQ"
      },
      "source": [
        "**<p>Environment Setting</p>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrnN1JzgMVZI"
      },
      "source": [
        "Assignment2State is an extension of the State class, adding attributes like goal_location, goal_direction, and item_direction. These additional attributes provide more context about the agent's environment, such as the direction to the goal and item. This increases our statespace size to 11 inputs to our DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m5aRXEOzKmpv"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class State:\n",
        "    # it doesn not hold AgentObject / ItemObject because I want State to be immutable\n",
        "    # but in the future, we might want to add more attributes to State\n",
        "    # in that case we need to make a copy of the AgentObject / ItemObject\n",
        "    agent_location: tuple[int, int]\n",
        "    item_location: tuple[int, int]\n",
        "    has_item: bool = False\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class Assignment2State(State):\n",
        "    goal_location: tuple[int, int]\n",
        "\n",
        "    # https://edstem.org/au/courses/17085/discussion/2192014\n",
        "    # these two attributes should be vectors (does not need to be unit vectors as we use cos distance)\n",
        "    # but these should be in terms of coordinates, not indices so be careful\n",
        "    goal_direction: tuple[float, float]\n",
        "    item_direction: tuple[float, float]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLMqLBCnjwOp"
      },
      "source": [
        "**<P>Environment Setup</p>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPI0C-j0jwOp"
      },
      "source": [
        "<p>The Assignment2Environment class is a wrapper for handling multiple sub-environments. Each sub-environment has different goal and item locations, providing more varied scenarios for training the agent (we chose this way of implementation to reuse the `Environment` class we already implemented in the previous assignment). The class manages the initialization of these environments and the state transitions between them, utilizing the extended state information provided by Assignment2State.\n",
        "\n",
        "<p>The Environment class itself has been enhanced to support more flexible initializations and interactions. It includes additional parameters for penalties and rewards and improved handling of animations for visualizing agent movements and decisions.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYA0AB8jwOq"
      },
      "source": [
        "**<p>Visualization</p>**\n",
        "\n",
        "<p>The visualization in the environment is set up to represent the agent's behavior within an n x n grid world. The agent (A) moves around the grid to pick up an item (I) and reach a goal (G), while the entire process is animated using Matplotlib. It also shows the Q-Values of state-action pair, for the very recent action.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lnhIAP2jwOq"
      },
      "source": [
        "**<p>Initialization:</p>**\n",
        "\n",
        "<ol>\n",
        "<li>The environment is initialized with parameters n (representing the size of the grid n x n) and with_animation (a boolean indicating whether to show animations or not).</li>\n",
        "<li>During the initialization, the initialize_for_new_episode function is called to create the plot using Matplotlib, and the animate function is called to set up the elements inside the plot.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JASnWqAtjwOq"
      },
      "source": [
        "**<p>animate() Function:</p>**\n",
        "Setup: Sets up a grid of size n x n and displays it using Matplotlib.  \n",
        "Icons:\n",
        "<ol>\n",
        "<li>A (Agent): Represents the \"person\" or the agent navigating through the grid.</li>\n",
        "<li>I (Item): Represents the \"item\" that the agent needs to collect.</li>\n",
        "<li>G (Goal): Represents the \"goal\" or the final destination that the agent needs to reach after collecting the item.</li>\n",
        "<li>Legend: A legend is displayed to help identify the icons (A, I, G) on the grid.</li>\n",
        "</ol>\n",
        "\n",
        "**<p>Positions:</p>**\n",
        "*   The goal (G) is now positioned randomly in the grid.\n",
        "\n",
        "**<p>Agent State Change:</p>**\n",
        "*   When the agent picks up the item (I), its color changes, indicating that it is now carrying the item. The legend is updated to reflect this change.\n",
        "\n",
        "**<p>Visual Updates:</p>**\n",
        "*   A small delay (0.7 seconds) is added to allow visualization of each movement or state change on the grid, providing a smooth animation experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAcrhKDZjwOq"
      },
      "source": [
        "**<p>step() Function</p>**\n",
        "\n",
        "*   This function is responsible for causing the movement of the agent within the grid.\n",
        "*   The agent selects an action (such as moving left, right, up, down, or collecting the item)\n",
        "*   The action is performed if it is valid (e.g., staying within grid boundaries), and the agent's state is updated accordingly.\n",
        "*   After the action is performed, the animate function is called again to display the new state of the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM_RFtk-m98Z"
      },
      "source": [
        "**<p>Reward Structure</p>**\n",
        "\n",
        "For the reward structure, please see the `get_reward` function of the `Environment` class. We give an item collection reward of `self.item_state_reward` when the agent collects the item for the first time by visiting item location correctly. `self.item_revisit_penalty` gives a penalty for revisiting the item position. We assign a large goal reward of `self.goal_state_reward` for reaching the goal state with the item, and we apply a penalty of `self.goal_no_item_penalty` if the agent reaches the goal without the item. The goal state is only valid when the item has been collected; reaching the goal before collecting the item results in a large penalty to discourage this. Apart from these cases, a time penalty of `self.time_penalty` is consistently applied to encourage efficiency in completing the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SHTjt8ydKo2o"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from abc import ABC\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "\n",
        "DEFAULT_TIME_PENALTY = -1\n",
        "GOAL_STATE_REWARD = 200\n",
        "DEFAULT_ITEM_REWARD = 300\n",
        "DEFAULT_ITEM_REVISIT_PENALTY = -200\n",
        "DEFAULT_GOAL_NO_ITEM_PENALTY = -300\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: int = 5,\n",
        "        item: ItemObject | None = None,\n",
        "        goal_location: tuple[int, int] = (4, 0),\n",
        "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
        "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
        "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
        "        item_revisit_penalty: int | float = DEFAULT_ITEM_REVISIT_PENALTY,\n",
        "        goal_no_item_penalty: int | float = DEFAULT_GOAL_NO_ITEM_PENALTY,\n",
        "        has_item_prob: float = 0.3,\n",
        "        with_animation: bool = True,\n",
        "    ) -> None:\n",
        "        self.n = n\n",
        "        self.goal_location = goal_location\n",
        "        self.time_penalty = time_penalty\n",
        "        self.item_state_reward = item_state_reward\n",
        "        self.goal_state_reward = goal_state_reward\n",
        "        self.item_revisit_penalty = item_revisit_penalty\n",
        "        self.goal_no_item_penalty = goal_no_item_penalty\n",
        "        self.has_item_prob = has_item_prob\n",
        "\n",
        "        self.item = ItemObject() if item is None else item\n",
        "        self.agent = AgentObject()\n",
        "\n",
        "        if self.item.location is None:\n",
        "            self.item.set_location_randomly(self.n, self.n)\n",
        "\n",
        "        self.state: State\n",
        "        # TODO: possibly implmeent this if there are multiple GridObjects to check for\n",
        "        # initialize grid and put grid objects on the grid\n",
        "        # x_agent, y_agent = self.agent.location\n",
        "        # x_item, y_item = self.item.location\n",
        "        # self.grid = np.zeros((self.n, self.n))\n",
        "        # self.grid[x_agent, y_agent] = self.agent\n",
        "        # self.grid[x_item, y_item] = self.item\n",
        "\n",
        "        # Setup for animation\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None) -> None:\n",
        "        if agent_location is None:\n",
        "            self.agent.set_location_randomly(self.n, self.n,) \n",
        "        else:\n",
        "            self.agent.location = agent_location\n",
        "        self.agent.has_item = True if random.random() < self.has_item_prob else False\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "        \n",
        "        # ensure that no multiple matplotlib windows open\n",
        "        if hasattr(self, \"fig\"):\n",
        "            plt.close(self.fig)  # type: ignore\n",
        "        self.fig, self.ax = plt.subplots(figsize=(8, 8)) if self.with_animation else (None, None)\n",
        "        self.animate()  # Initial drawing of the grid\n",
        "\n",
        "        # Reset the last action and reward\n",
        "        self.last_action = None\n",
        "        self.last_reward = None\n",
        "\n",
        "\n",
        "    def get_state(self) -> State:\n",
        "        return self.state\n",
        "    \n",
        "    def set_with_animation(self, with_animation: bool) -> None:\n",
        "        self.with_animation = with_animation\n",
        "\n",
        "    def get_available_actions(self, state: State | None = None) -> list[Action]:\n",
        "        \"\"\"\n",
        "        Assumes that the current state is not the goal state\n",
        "        \"\"\"\n",
        "        # logic to determine available actions\n",
        "        actions = []\n",
        "        current_state = state if state is not None else self.state\n",
        "        x, y = current_state.agent_location\n",
        "\n",
        "        if current_state.agent_location == current_state.item_location and not current_state.has_item:\n",
        "            actions.append(Action.COLLECT)\n",
        "\n",
        "        # note: technically speaking we know that whenever agent is at the item location, the only available (or, the most optimal) action is to collect the item\n",
        "        # however, according to the CE, we must ensure that\n",
        "        # \"the agent is supposed to learn (rather than being told) that\n",
        "        # once it has picked up the load it needs to move to the delivery point to complete its mission. \",\n",
        "        # implyging that agent must be able to learn to \"collect\" instead of being told to collect (so add all possible actions)\n",
        "        if x > 0:\n",
        "            actions.append(Action.LEFT)  # left\n",
        "        if x < self.n - 1:\n",
        "            actions.append(Action.RIGHT)  # right\n",
        "        if y > 0:\n",
        "            actions.append(Action.DOWN)  # down\n",
        "        if y < self.n - 1:\n",
        "            actions.append(Action.UP)  # up\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def get_reward(self, prev_state: State, current_state: State, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the reward based on the agent's actions and state transitions.\n",
        "        \"\"\"\n",
        "        reward = self.time_penalty\n",
        "        \n",
        "        # Large penalty for reaching the goal without the item\n",
        "        if current_state.agent_location == self.goal_location and not current_state.has_item:\n",
        "            reward += self.goal_no_item_penalty\n",
        "            return reward\n",
        "\n",
        "        # Large reward for reaching the goal with the item\n",
        "        if self.is_goal_state(current_state):\n",
        "            reward += self.goal_state_reward\n",
        "\n",
        "        # Reward for collecting the item\n",
        "        if not prev_state.has_item and current_state.agent_location == current_state.item_location:\n",
        "            reward += self.item_state_reward\n",
        "        if action == Action.COLLECT and prev_state.agent_location == current_state.item_location and not prev_state.has_item:\n",
        "            reward += self.item_state_reward\n",
        "\n",
        "        # Penalty for revisiting item location\n",
        "        if action == Action.COLLECT and (prev_state.has_item or prev_state.agent_location != current_state.item_location):\n",
        "            reward += self.item_revisit_penalty\n",
        "        if prev_state.has_item and current_state.agent_location == current_state.item_location:\n",
        "            reward += self.item_revisit_penalty\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def update_state(self, action: Action) -> None:\n",
        "        \"\"\"\n",
        "        Be careful: this method updates the state of the environment\n",
        "        \"\"\"\n",
        "        self.agent.move(action,self.n)\n",
        "        self.state = State(\n",
        "            agent_location=self.agent.get_location(),\n",
        "            item_location=self.item.get_location(),\n",
        "            has_item=self.agent.has_item,\n",
        "        )\n",
        "\n",
        "    def is_goal_state(self, state: State) -> bool:\n",
        "        return self.state.has_item and self.goal_location == state.agent_location\n",
        "\n",
        "    def animate(self, state: Assignment2State | None = None, prev_state: Assignment2State | None = None, is_greedy: bool | None = None, all_qvals: np.ndarray | None = None, chosen_action: Action | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Animates the action\n",
        "        (basically just prints out the new state, but because it seems like the agent is \"moving\" because it's updated in the same figure)\n",
        "        \"\"\"\n",
        "        if not self.with_animation:\n",
        "            return\n",
        "        self.ax.clear()\n",
        "        self.ax.set_xlim(0, self.n)\n",
        "        self.ax.set_ylim(0, self.n)\n",
        "        self.ax.set_xticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.set_yticks(np.arange(0, self.n + 1, 1))\n",
        "        self.ax.grid(True)\n",
        "\n",
        "        # Plotting the agent, item, and goal\n",
        "        self.ax.text(\n",
        "            self.agent.location[0] + 0.5,\n",
        "            self.agent.location[1] + 0.5,\n",
        "            \"A\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"blue\" if not self.agent.has_item else \"purple\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.item.location[0] + 0.5,\n",
        "            self.item.location[1] + 0.5,\n",
        "            \"I\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"green\",\n",
        "        )\n",
        "        self.ax.text(\n",
        "            self.goal_location[0] + 0.5,\n",
        "            self.goal_location[1] + 0.5,\n",
        "            \"G\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=16,\n",
        "            color=\"red\",\n",
        "        )\n",
        "        \n",
        "        # FIXME: this doesn't work for State (only works for Assignment2State)\n",
        "        state_str = str(self.state) if state is None else str(state)\n",
        "        state_text = \"\".join(state_str.split(',')[:5]) + '\\n' + \"\".join(state_str.split(',')[5:])\n",
        "        \n",
        "        # show state info\n",
        "        self.ax.text(\n",
        "            2,\n",
        "            2,\n",
        "            state_text,\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=10,\n",
        "            color=\"orange\",\n",
        "        )\n",
        "        \n",
        "        # prints: if the action selected was greedy or random\n",
        "        if is_greedy is not None:\n",
        "            self.ax.text(\n",
        "            self.n,\n",
        "            self.n,\n",
        "            \"Action is greedy\" if is_greedy else \"Action is random\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=10,\n",
        "            color=\"black\",\n",
        "        )\n",
        "        \n",
        "        # prints the q values for all possible actions in the previous state\n",
        "        # note: this is only printed if the action was greedy (because if random, the q values did not matter for action selection)\n",
        "        # note2: only \"possible\" actions are printed i.e. (if agent is not at the item position, it does not print the collect q value)\n",
        "        if all_qvals is not None and prev_state is not None and is_greedy:\n",
        "            left_q, right_q, down_q, up_q, collect_q = all_qvals\n",
        "            possible_actions = self.get_available_actions(prev_state)\n",
        "            # show left q value\n",
        "            prev_agent_location_on_plot_x = prev_state.agent_location[0] + 0.5\n",
        "            prev_agent_location_on_plot_y = prev_state.agent_location[1] + 0.5\n",
        "            box_center_to_val_location = 0.3\n",
        "            if Action.LEFT in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x - box_center_to_val_location,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{left_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.LEFT else \"black\",\n",
        "                )\n",
        "            if Action.RIGHT in possible_actions:    \n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x + box_center_to_val_location,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{right_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.RIGHT else \"black\",\n",
        "                )\n",
        "            if Action.DOWN in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y - box_center_to_val_location,\n",
        "                    f'{down_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.DOWN else \"black\",\n",
        "                )\n",
        "            if Action.UP in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y + box_center_to_val_location,\n",
        "                    f'{up_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.UP else \"black\",\n",
        "                )\n",
        "            if Action.COLLECT in possible_actions:\n",
        "                self.ax.text(\n",
        "                    prev_agent_location_on_plot_x,\n",
        "                    prev_agent_location_on_plot_y,\n",
        "                    f'{collect_q:.2f}',\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    fontsize=13,\n",
        "                    color=\"red\" if chosen_action == Action.COLLECT else \"black\",\n",
        "                )\n",
        "            \n",
        "                \n",
        "\n",
        "        # TODO: add a message saying \"item collected\" if the agent has collected the item\n",
        "        # or else there is a single frame where the agent is at the same location twice,\n",
        "        # so it looks like the agent is not moving\n",
        "        handles = [\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"blue\", markersize=8, label=\"Agent (A)\")\n",
        "            if not self.agent.has_item\n",
        "            else plt.Line2D(\n",
        "                [0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"purple\", markersize=8, label=\"Agent (A) with item\"\n",
        "            ),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"green\", markersize=8, label=\"Item (I)\"),\n",
        "            plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"red\", markersize=8, label=\"Goal (G)\"),\n",
        "        ]\n",
        "        self.ax.legend(handles=handles, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "        plt.subplots_adjust(right=0.75, left=0.1)\n",
        "        self.fig.canvas.draw_idle()\n",
        "        plt.pause(0.7)  # Pause to allow visualization of the movement\n",
        "\n",
        "    def step(self, action: Action) -> tuple[float, State]:\n",
        "        prev_state = self.get_state()\n",
        "        self.update_state(action)\n",
        "        next_state = self.get_state()\n",
        "        self.animate()\n",
        "        reward = self.get_reward(prev_state, next_state,action)\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "class Assignment2Environment:\n",
        "    \"\"\"\n",
        "    A wrapper class for multiple environments for Assignment 2\n",
        "    This environment consits of multiple \"sub-environments\" where each sub-environment has a different goal and item location\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        n: int = 5,\n",
        "        time_penalty: int | float = DEFAULT_TIME_PENALTY,\n",
        "        item_state_reward: int | float = DEFAULT_ITEM_REWARD,\n",
        "        goal_state_reward: int | float = GOAL_STATE_REWARD,\n",
        "        item_revisit_penalty: int | float = DEFAULT_ITEM_REVISIT_PENALTY,\n",
        "        goal_no_item_penalty: int | float = DEFAULT_GOAL_NO_ITEM_PENALTY,\n",
        "        direction_reward_multiplier: int | float = 10,\n",
        "        with_animation: bool = True,\n",
        "    ) -> None:\n",
        "        self.n = n\n",
        "        # initialize a list of environments for all possible goal and item positions\n",
        "        self.environments = []\n",
        "        \n",
        "        for goal_x in range(self.n):\n",
        "            for goal_y in range(self.n):\n",
        "                for item_x in range(self.n):\n",
        "                    for item_y in range(self.n):\n",
        "                        if (goal_x, goal_y) == (item_x, item_y):\n",
        "                            continue\n",
        "                        environment = Environment(\n",
        "                            n=self.n,\n",
        "                            goal_location=(goal_x, goal_y),\n",
        "                            item=ItemObject(location=(item_x, item_y)),\n",
        "                            with_animation=with_animation,\n",
        "                            time_penalty=time_penalty,\n",
        "                            item_state_reward=item_state_reward,\n",
        "                            goal_state_reward=goal_state_reward,\n",
        "                            item_revisit_penalty=item_revisit_penalty,\n",
        "                            goal_no_item_penalty=goal_no_item_penalty,\n",
        "                        )\n",
        "                        self.environments.append(environment)\n",
        "        \n",
        "        # self.environments = [self.environments[10]]\n",
        "                \n",
        "        \n",
        "        self.direction_reward_multiplier = direction_reward_multiplier\n",
        "        \n",
        "        self.current_sub_environment: Environment\n",
        "        self.state: Assignment2State\n",
        "    \n",
        "    def get_random_sub_environment(self) -> Environment:\n",
        "        return random.choice(self.environments)\n",
        "    \n",
        "    def initialize_for_new_episode(self, agent_location: tuple[int, int] | None = None, env_index: int | None = None) -> None:\n",
        "        self.current_sub_environment = self.get_random_sub_environment() if env_index is None else self.environments[env_index]\n",
        "        self.current_sub_environment.initialize_for_new_episode(agent_location)\n",
        "        \n",
        "        self.state = Assignment2State(\n",
        "            agent_location=self.current_sub_environment.agent.get_location(),\n",
        "            item_location=self.current_sub_environment.item.get_location(),\n",
        "            has_item=self.current_sub_environment.agent.has_item,\n",
        "            goal_location=self.current_sub_environment.goal_location,\n",
        "            goal_direction=self.get_goal_direction(),\n",
        "            item_direction=self.get_item_direction(),\n",
        "        )\n",
        "        # NOTE: animation should be handled by individual sub-environments\n",
        "    \n",
        "    def get_available_actions(self, state:Assignment2State) -> list[Action]:\n",
        "        return self.current_sub_environment.get_available_actions(state)\n",
        "\n",
        "    def set_with_animation(self, with_animation: bool) -> None:\n",
        "        for environment in self.environments:\n",
        "            environment.set_with_animation(with_animation)\n",
        "    \n",
        "    def get_direction_reward(self, action: Action) -> float:\n",
        "        \"\"\"\n",
        "        Use cosine similarity to calculate the reward based on the direction of the action.\n",
        "        \"\"\"\n",
        "        has_collected_item = self.state.has_item\n",
        "\n",
        "        # Define action direction vectors\n",
        "        if action == Action.LEFT:\n",
        "            action_direction = (-1, 0)\n",
        "        elif action == Action.RIGHT:\n",
        "            action_direction = (1, 0)\n",
        "        elif action == Action.DOWN:\n",
        "            action_direction = (0, -1)\n",
        "        elif action == Action.UP:\n",
        "            action_direction = (0, 1)\n",
        "        else:\n",
        "            action_direction = (0, 0)  # Invalid action, handle accordingly\n",
        "\n",
        "        # Calculate the direction reward based on the goal or item direction\n",
        "        if has_collected_item:\n",
        "            target_direction = self.state.goal_direction\n",
        "        else:\n",
        "            target_direction = self.state.item_direction\n",
        "\n",
        "        # Check if either vector is zero to avoid division by zero\n",
        "        if np.linalg.norm(action_direction) == 0 or np.linalg.norm(target_direction) == 0:\n",
        "            return 0.0  # No direction reward if either vector is zero\n",
        "\n",
        "        # Calculate the cosine similarity (1 - cosine distance)\n",
        "        try:\n",
        "            reward = 1 - cosine(action_direction, target_direction)\n",
        "        except ValueError:\n",
        "            # Handle any errors from invalid vectors\n",
        "            reward = 0.0\n",
        "\n",
        "        return reward * self.direction_reward_multiplier\n",
        "\n",
        "    \n",
        "    # def get_reward(self, prev_state: Assignment2State, current_state: Assignment2State, action: Action) -> float:\n",
        "    #     state_raward = self.current_sub_environment.get_reward(prev_state, current_state,action)\n",
        "    #     action_reward = self.get_direction_reward(action)\n",
        "    #     return state_raward + action_reward\n",
        "    def get_reward(self, prev_state: Assignment2State, current_state: Assignment2State, action: Action) -> float:\n",
        "        state_raward = self.current_sub_environment.get_reward(prev_state, current_state, action)\n",
        "        return state_raward\n",
        "    \n",
        "    \n",
        "    def get_state(self) -> Assignment2State:\n",
        "        return self.state\n",
        "    \n",
        "    def get_goal_direction(self) -> tuple[float, float]:\n",
        "        return (\n",
        "            self.current_sub_environment.goal_location[0] - self.current_sub_environment.agent.get_location()[0],\n",
        "            self.current_sub_environment.goal_location[1] - self.current_sub_environment.agent.get_location()[1],\n",
        "        )\n",
        "    \n",
        "    def get_item_direction(self) -> tuple[float, float]:\n",
        "        return (\n",
        "            self.current_sub_environment.item.get_location()[0] - self.current_sub_environment.agent.get_location()[0],\n",
        "            self.current_sub_environment.item.get_location()[1] - self.current_sub_environment.agent.get_location()[1],\n",
        "        )\n",
        "    \n",
        "    def is_goal_state(self, state: State) -> bool:\n",
        "        return self.current_sub_environment.state.has_item and self.current_sub_environment.goal_location == state.agent_location\n",
        "    \n",
        "    def update_state(self, action: Action) -> None:\n",
        "        \"\"\"\n",
        "        Be careful: this method updates the state of the environment\n",
        "        \"\"\"\n",
        "        self.current_sub_environment.update_state(action)\n",
        "        self.state = Assignment2State(\n",
        "            agent_location=self.current_sub_environment.agent.get_location(),\n",
        "            item_location=self.current_sub_environment.item.get_location(),\n",
        "            has_item=self.current_sub_environment.agent.has_item,\n",
        "            goal_location=self.current_sub_environment.goal_location,\n",
        "            goal_direction=self.get_goal_direction(),\n",
        "            item_direction=self.get_item_direction(),\n",
        "        )\n",
        "    \n",
        "    def step(self, action: Action, is_greedy: bool, all_qvals: np.ndarray) -> tuple[float, Assignment2State]:\n",
        "        prev_state = self.get_state()\n",
        "        self.update_state(action)\n",
        "        next_state = self.get_state()\n",
        "        self.current_sub_environment.animate(self.get_state(), prev_state, is_greedy, all_qvals, action)\n",
        "        reward = self.get_reward(prev_state, next_state, action)\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "class GridObject(ABC):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        self.icon: str\n",
        "        self.location = (\n",
        "            location  # NOTE: location is a tuple of (x, y) where x and y are coordinates on the grid (not indices)\n",
        "        )\n",
        "\n",
        "    def set_location_randomly(\n",
        "        self, max_x: int, max_y: int, disallowed_locations: list[tuple[int, int]] = []\n",
        "    ) -> tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Note: max_x and max_y are exclusive\n",
        "\n",
        "        disallowed_locations: list of locations that are not allowed to be placed\n",
        "        (e.g. agent and item location should not be initialized to the same place)\n",
        "        \"\"\"\n",
        "        # The start, item, goal location must be different position\n",
        "        location = None\n",
        "        while location is None or location in disallowed_locations:\n",
        "            location = (random.randint(0, max_x - 1), random.randint(0, max_y - 1))\n",
        "\n",
        "        self.location = location\n",
        "        return location\n",
        "\n",
        "    def get_location(self) -> tuple[int, int]:\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Location is not set\")\n",
        "        return self.location\n",
        "\n",
        "\n",
        "class AgentObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None) -> None:\n",
        "        super().__init__(location)\n",
        "        self.icon = \"A\"\n",
        "        self.has_item = False  # TODO: has_item of AgentObject and State must be synched somehow\n",
        "\n",
        "    def move(self, action: Action, grid_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Move the agent based on the given action while ensuring it doesn't leave the bounds of the grid.\n",
        "        \"\"\"\n",
        "        if self.location is None:\n",
        "            raise ValueError(\"Agent location is not set\")\n",
        "\n",
        "        x, y = self.location\n",
        "\n",
        "        # Check each action and ensure it stays within the bounds\n",
        "        if action == Action.LEFT:\n",
        "            if x > 0:  # Ensure not moving out of bounds on the left\n",
        "                self.location = (x - 1, y)\n",
        "        elif action == Action.RIGHT:\n",
        "            if x < grid_size - 1:  # Ensure not moving out of bounds on the right\n",
        "                self.location = (x + 1, y)\n",
        "        elif action == Action.DOWN:\n",
        "            if y > 0:  # Ensure not moving out of bounds downwards\n",
        "                self.location = (x, y - 1)\n",
        "        elif action == Action.UP:\n",
        "            if y < grid_size - 1:  # Ensure not moving out of bounds upwards\n",
        "                self.location = (x, y + 1)\n",
        "        elif action == Action.COLLECT:\n",
        "            self.has_item = True  # Action to collect the item (no bounds check needed)\n",
        "\n",
        "\n",
        "\n",
        "class ItemObject(GridObject):\n",
        "    def __init__(self, location: tuple[int, int] | None = None):\n",
        "        super().__init__(location)\n",
        "        self.icon = \"I\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrtXxxo2Tq2Y"
      },
      "source": [
        "**<p>Replay Buffer</p>**\n",
        "\n",
        "<p>BaseReplayBuffer, a base replay buffer defined to provide a common interface and basic functionality such as a deque that stores experiences and the maximum number of experiences that a buffer can hold. The remember function will add new experiences to the buffer and remove the oldest experience once the buffer reached its maximum size. The abstract method is intended to be implemented by subclasses to sample a batch of experiences from the buffer. It provides a way to retrieve multiple experiences at once for training the DQN agent.</p>\n",
        "\n",
        "<p> ReplayBuffer aims to sample a random subset of experiences from the buffer without replacement and unpacks the sampled experiences into separate lists for states, actions, rewards, next states and done flags using zip function.</p>\n",
        "\n",
        "<p>The object function PrioritizedExperienceBuffer extends BaseReplayBuffer to implement prioritized experience replay. This technique assigns higher priorities to the sample in which the model is performing bad i.e. high TD target. We use an alpha value hyperparameter to control the prioritzation of experiences. It affects the degree to which priorities influence sampling probabilities.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tEu0uvTkEDYx"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Experience = tuple[np.ndarray, int, float, np.ndarray, bool]\n",
        "\n",
        "\n",
        "class BaseReplayBuffer(ABC):\n",
        "    def __init__(self, max_size: int):\n",
        "        self.max_size = max_size\n",
        "        self.buffer: deque[Experience] = deque([])\n",
        "\n",
        "    def remember(self, experience: Experience) -> None:\n",
        "        if len(self.buffer) >= self.max_size:\n",
        "            self.buffer.popleft()  # Remove the oldest experience\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    @abstractmethod\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ReplayBuffer(BaseReplayBuffer):\n",
        "    def __init__(self, max_size: int):\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences from the replay memory.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
        "        return (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            done_batch,\n",
        "        )\n",
        "\n",
        "\n",
        "class PrioritizedExperienceBuffer(BaseReplayBuffer):\n",
        "    def __init__(self, max_size: int, alpha: float = 0.6):\n",
        "        \"\"\"\n",
        "        alpha for prioritization\n",
        "        \"\"\"\n",
        "        super().__init__(max_size)\n",
        "        self.alpha = alpha\n",
        "        self.priorities = np.ones(max_size, dtype=np.float32)\n",
        "\n",
        "        self.sampled_indices: np.ndarray[int] | None = None\n",
        "\n",
        "    def remember(self, experience: Experience) -> None:\n",
        "        if len(self.buffer) >= self.max_size:\n",
        "            self.buffer.popleft()\n",
        "            self.priorities[:-1] = self.priorities[1:]\n",
        "\n",
        "        self.buffer.append(experience)\n",
        "        max_priority = self.priorities.max() if self.buffer else 1.0  # max to ensure newly added experience has the highest priority\n",
        "        self.priorities[len(self.buffer) - 1] = max_priority\n",
        "\n",
        "    def sample_batch(self, batch_size: int) -> tuple[list[np.ndarray], list[int], list[float], list[np.ndarray], list[bool]]:\n",
        "        \"\"\"\n",
        "        Sample a batch of experiences from the replay memory.\n",
        "        \"\"\"\n",
        "        priorities = self.priorities[: len(self.buffer)]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        self.sampled_indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        samples = [self.buffer[idx] for idx in self.sampled_indices]\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*samples)\n",
        "        return (\n",
        "            state_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_state_batch,\n",
        "            done_batch,\n",
        "        )\n",
        "\n",
        "    def update_priorities(self, priorities: np.ndarray) -> None:\n",
        "        if self.sampled_indices is None:\n",
        "            raise ValueError(\"You need to sample a batch before updating priorities\")\n",
        "        for idx, priority in zip(self.sampled_indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "        self.sampled_indices = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvVZJdBE8kY"
      },
      "source": [
        "**<p>DQN Agent that utilise Deep Q Network</p>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**<p>Initialization (__init__):</p>**\n",
        "*   The constructor sets up hyperparameters for the DQN agent such as learning rate (alpha), discount factor (discount_rate), exploration parameters (epsilon, epsilon_decay, epsilon_min), replay memory settings, and batch size.\n",
        "*   It initializes a prediction model and a target model using neural networks, which are crucial for the DQN algorithm.\n",
        "*   The agent uses an optimizer (AdamW) for training the model and a learning rate scheduler to adjust the learning rate dynamically.\n",
        "*   A replay buffer is created to store past experiences, enabling experience replay to stabilize training.\n",
        "\n",
        "**<p>Neural Network Preparation (prepare_torch):</p>**\n",
        "*   Defines a neural network model using PyTorch for approximating the Q-value function. The model consists of input, hidden, and output layers with ReLU activations.\n",
        "\n",
        "**<p>Target Network Update (update_target_network):</p>**\n",
        "*   Updates the weights of the target network using a soft update mechanism (tau) to keep it slowly in sync with the prediction network. This improves stability during training.\n",
        "\n",
        "**<p>Action Selection (select_action):</p>**\n",
        "*   Implements an Îµ-greedy policy for action selection. The agent explores with a certain probability (epsilon) or exploits the best-known action based on the Q-values from the prediction model.\n",
        "\n",
        "**<p>Q-Value Calculation (get_qvals, get_maxQ, get_double_q):</p>**\n",
        "*   Computes Q-values for given states using both the prediction and target networks to determine the optimal actions and targets for training.\n",
        "\n",
        "**<p>Training (train_one_step, replay):</p>**\n",
        "*   train_one_step: Performs a single step of gradient descent on the prediction network to minimize the difference between predicted and target Q-values. We try both the basic TD target (which uses the the maximum q value of a state of the target network) and Double Q target (which first selects the action using policy network then gets its q value using the target network) to avoid overestimation of q values.\n",
        "*   replay: Samples a batch of experiences from the replay buffer and uses them to train the model, improving its decision-making policy.\n",
        "Model Saving and Loading (save_state, load_state):\n",
        "\n",
        "**<p>Model Saving and Loading (save_state, load_state):</p>**\n",
        "*   Allows saving and loading of the agentâ€™s state, including model weights, optimizer state, exploration parameters, replay buffer, and random seeds, enabling the agent to be resumed from a previous state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "unppTePzKo3a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        statespace_size: int = 11,\n",
        "        action_space_size: int = len(Action),\n",
        "        alpha: float = 0.0005,\n",
        "        discount_rate: float = 0.99,\n",
        "        epsilon: float = 1,\n",
        "        epsilon_decay: float = 0.99997,\n",
        "        epsilon_min: float = 0.007,\n",
        "        replay_memory_size: int = 10000,\n",
        "        batch_size: int = 128,\n",
        "        min_replay_memory_size: int = 1000,\n",
        "        tau: float = 0.05,\n",
        "        with_log: bool = False,\n",
        "        loss_log_interval: int = 100,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the DQN Agent\n",
        "        \"\"\"\n",
        "        self.alpha = alpha  # learning rate for optimizer\n",
        "        self.discount_rate = discount_rate\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # rate at which exploration rate decays\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.min_replay_memory_size = min_replay_memory_size\n",
        "        \n",
        "        self.replay_buffer = PrioritizedExperienceBuffer(max_size=replay_memory_size)\n",
        "        \n",
        "        # Initialize DQN models\n",
        "        self.model = self.prepare_torch(statespace_size)  # prediction model\n",
        "        self.target_model = deepcopy(self.model)  # target model\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.alpha, amsgrad=True)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.9)\n",
        "        self.loss_fn = torch.nn.MSELoss(reduction='none')\n",
        "        self.steps = 0\n",
        "        \n",
        "        self.tau = tau  # for soft update of target parameters\n",
        "\n",
        "        # Internal data tracking for plotting purposes\n",
        "        self.logged_data = {\n",
        "            'avg_predicted_qval': [],\n",
        "            'avg_target_qval': [],\n",
        "            'max_predicted_qval': [],\n",
        "            'max_target_qval': [],\n",
        "            'loss': [],\n",
        "            'steps': []\n",
        "        }\n",
        "        \n",
        "        self.with_log = with_log\n",
        "        self.loss_log_interval = loss_log_interval\n",
        "\n",
        "    def prepare_torch(self, statespace_size: int):\n",
        "        \"\"\"\n",
        "        Prepare the PyTorch model for DQL.\n",
        "        \"\"\"\n",
        "        l1 = statespace_size\n",
        "        l2 = 150\n",
        "        l3 = 100\n",
        "        l4 = self.action_space_size\n",
        "        model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(l1, l2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(l2, l3),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(l3, l4)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self) -> None:\n",
        "        \"\"\"\n",
        "        Copy weights from the prediction network to the target network.\n",
        "        \"\"\"\n",
        "        # self.target_model = deepcopy(self.model)\n",
        "        target_net_state_dict = self.target_model.state_dict()\n",
        "        policy_net_state_dict = self.model.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
        "        self.target_model.load_state_dict(target_net_state_dict)\n",
        "\n",
        "    def select_action(self, state: np.ndarray, available_actions: List[Action], is_test: bool = False) -> tuple[Action, bool, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Select an action using an Îµ-greedy policy.\n",
        "        \n",
        "        second return val for is_greedy\n",
        "        chosen_action, is_greedy, q values for all actions\n",
        "        \"\"\"\n",
        "        qvals = self.get_qvals(state)\n",
        "        if not is_test and random.random() < self.epsilon:\n",
        "            return random.choice(available_actions), False, qvals\n",
        "        else:\n",
        "            # Filter Q-values to only consider available actions\n",
        "            valid_qvals = [qvals[action.value] for action in available_actions]\n",
        "            return available_actions[np.argmax(valid_qvals)], True, qvals\n",
        "        \n",
        "    def get_qvals(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get Q-values for a given state from the prediction network.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)  # Convert to tensor\n",
        "        with torch.no_grad():\n",
        "            qvals_tensor = self.model(state_tensor)\n",
        "        return qvals_tensor.detach().numpy()[0]\n",
        "\n",
        "    def get_maxQ(self, state: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Get the maximum Q-value for a given state from the target network.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)  # Convert to tensor\n",
        "        with torch.no_grad():\n",
        "            max_qval_tensor = torch.max(self.target_model(state_tensor))\n",
        "        return max_qval_tensor.item()\n",
        "    \n",
        "    def get_double_q(self, state: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the Double DQN target value for a given state.\n",
        "        \"\"\"\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            best_action_index = torch.argmax(self.model(state_tensor)[0]).item()\n",
        "            max_qval = self.target_model(state_tensor)[0][best_action_index].item()\n",
        "            \n",
        "        return max_qval\n",
        "\n",
        "    def train_one_step(self, states: List[np.ndarray], actions: List[int], targets: List[float]) -> float:\n",
        "        \"\"\"\n",
        "        Perform a single training step on the prediction network.\n",
        "        \"\"\"\n",
        "        # Convert states, actions, and targets to tensors\n",
        "        state_tensors = torch.cat([torch.from_numpy(s).float().unsqueeze(0) for s in states])\n",
        "        action_tensors = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
        "        target_tensors = torch.tensor(targets, dtype=torch.float)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        qvals = self.model(state_tensors).gather(1, action_tensors).squeeze()\n",
        "        losses = self.loss_fn(qvals, target_tensors)\n",
        "        loss = losses.mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        \n",
        "\n",
        "        # Logging the metrics internally\n",
        "        if self.with_log and self.steps % self.loss_log_interval == 0:\n",
        "            avg_predicted_qval = qvals.mean().item()\n",
        "            avg_target_qval = target_tensors.mean().item()\n",
        "            max_predicted_qval = qvals.max().item()\n",
        "            max_target_qval = target_tensors.max().item()\n",
        "\n",
        "            # Append to the internal tracking lists\n",
        "            self.logged_data['avg_predicted_qval'].append(avg_predicted_qval)\n",
        "            self.logged_data['avg_target_qval'].append(avg_target_qval)\n",
        "            self.logged_data['max_predicted_qval'].append(max_predicted_qval)\n",
        "            self.logged_data['max_target_qval'].append(max_target_qval)\n",
        "            self.logged_data['loss'].append(loss.item())\n",
        "            self.logged_data['steps'].append(self.steps)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            self.replay_buffer.update_priorities(losses.detach().numpy())  # TODO: maybe using l1 better (at least original paper uses l1)\n",
        "            \n",
        "        return loss.item()\n",
        "\n",
        "    def replay(self) -> None:\n",
        "        \"\"\"\n",
        "        Train the model using experience replay.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer.buffer) < self.min_replay_memory_size:\n",
        "            return\n",
        "        \n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample_batch(self.batch_size)\n",
        "\n",
        "        # Compute targets\n",
        "        targets = []\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                targets.append(rewards[i])\n",
        "            else:\n",
        "                # max_future_q = self.get_maxQ(next_states[i])\n",
        "                max_future_q = self.get_double_q(next_states[i])\n",
        "                targets.append(rewards[i] + self.discount_rate * max_future_q)\n",
        "\n",
        "        self.steps += 1\n",
        "        # Train the model\n",
        "        loss = self.train_one_step(states, actions, targets)\n",
        "\n",
        "        # TODO: plot loss\n",
        "\n",
        "    def save_state(self, filepath: Path | str):\n",
        "        \"\"\"Save the entire agent state, including model weights and hyperparameters.\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),  # Model weights\n",
        "            'target_model_state_dict': self.target_model.state_dict(),  # Target model weights\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),  # Optimizer state\n",
        "            'epsilon': self.epsilon,  # Epsilon value\n",
        "            'epsilon_decay': self.epsilon_decay,  # Epsilon decay rate\n",
        "            'epsilon_min': self.epsilon_min,  # Minimum epsilon\n",
        "            'discount_rate': self.discount_rate,  # Discount factor\n",
        "            'replay_buffer': self.replay_buffer,  # replay_buffer\n",
        "            'steps': self.steps,  # Steps to update target network\n",
        "            'random_state': random.getstate(),  # Python random state\n",
        "            'numpy_random_state': np.random.get_state(),  # Numpy random state\n",
        "        }, filepath)\n",
        "\n",
        "    def load_state(self, filepath):\n",
        "        \"\"\"Load the entire agent state, including model weights and hyperparameters.\"\"\"\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights\n",
        "        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])  # Load target model weights\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Restore optimizer state\n",
        "        self.epsilon = checkpoint['epsilon']  # Restore epsilon value\n",
        "        self.epsilon_decay = checkpoint['epsilon_decay']  # Restore epsilon decay rate\n",
        "        self.epsilon_min = checkpoint['epsilon_min']  # Restore minimum epsilon\n",
        "        self.discount_rate = checkpoint['discount_rate']  # Restore discount factor\n",
        "        self.replay_buffer = checkpoint['replay_buffer']  # Restore replay_buffer\n",
        "        self.steps = checkpoint['steps']  # Restore steps\n",
        "        random.setstate(checkpoint['random_state'])  # Restore Python random state\n",
        "        np.random.set_state(checkpoint['numpy_random_state'])  # Restore Numpy random state\n",
        "        # If using a learning rate scheduler:\n",
        "        # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])  # Restore scheduler state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Plotter class is designed to visualize and track the performance metrics of a Deep Q-Network (DQN) agent during training. It provides real-time plotting of key metrics such as the average predicted Q-values, target Q-values, and training loss over time. This visualization helps in understanding how well the DQN agent is learning and adapting its policy.\n",
        "Key Features:\n",
        "<li>Initialization: Sets up the plotting environment and creates a directory to save the generated plots.</li>\n",
        "<li>update_plot: Updates the plots dynamically with the latest logged data, including average Q-values and loss, ensuring that the plots reflect the most recent training progress. </li>\n",
        "<li>save_plot: Saves the updated plots as image files in the specified directory, allowing for easy reference and comparison of the agent's learning performance at different stages of training. </li>\n",
        "The Plotter class provides a convenient way to monitor and evaluate the training process of a DQN agent visually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, save_dir: str = \"./plots\"):\n",
        "        \"\"\"\n",
        "        Initialize the Plotter with the DQNAgent instance and refresh interval.\n",
        "        \n",
        "        :param agent: Instance of the DQNAgent to fetch data from.\n",
        "        :param refresh_interval: Time interval (in milliseconds) to refresh the plot.\n",
        "        :param save_dir: Directory where plots will be saved.\n",
        "        \"\"\"\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # Create the save directory if it doesn't exist\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        # Set up the plot\n",
        "        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "        self.fig.suptitle('DQN Agent Metrics Tracking')\n",
        "\n",
        "    def update_plot(self,logged_data):\n",
        "        \"\"\"\n",
        "        Update the plot with new data.\n",
        "        \"\"\"\n",
        "        # Fetch the latest metrics from the agent\n",
        "        \n",
        "        data = logged_data\n",
        "        \n",
        "        \n",
        "        # Clear the previous plots\n",
        "        self.ax1.clear()\n",
        "        self.ax2.clear()\n",
        "\n",
        "        # Plot average predicted Q-values and target Q-values\n",
        "        if data['avg_predicted_qval'] and data['avg_target_qval']:\n",
        "            self.ax1.plot(data['steps'], data['avg_predicted_qval'], label='Avg Predicted Q-Value', color='blue')\n",
        "            self.ax1.plot(data['steps'], data['avg_target_qval'], label='Avg Target Q-Value', color='green')\n",
        "            self.ax1.set_xlabel('Step')\n",
        "            self.ax1.set_ylabel('Q-Value')\n",
        "            self.ax1.set_title('Average Q-Values over Steps')\n",
        "            self.ax1.legend()\n",
        "\n",
        "        # Plot loss\n",
        "        if data['loss']:\n",
        "            self.ax2.plot(data['steps'], data['loss'], label='Loss', color='red')\n",
        "            self.ax2.set_xlabel('Step')\n",
        "            self.ax2.set_ylabel('Loss')\n",
        "            self.ax2.set_title('Training Loss over Steps')\n",
        "            self.ax2.legend()\n",
        "\n",
        "        # Refresh the plot\n",
        "        self.fig.canvas.draw()\n",
        "\n",
        "        # Save the plot to a file\n",
        "        self.save_plot()\n",
        "\n",
        "    def save_plot(self):\n",
        "        \"\"\"\n",
        "        Save the current plot to a file in the specified directory.\n",
        "        \"\"\"\n",
        "        filename = os.path.join(self.save_dir, f\"agent_metrics.png\")\n",
        "        self.fig.savefig(filename)\n",
        "        print(f\"Plot saved to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cyt3tcaEm8M"
      },
      "source": [
        "**<p>Trainer that will be training the model</p>**\n",
        "\n",
        "Learning Visualization:\n",
        "\n",
        "*   Early Stages: In the early stages of training, the agent performs random actions to explore the environment, which can result in the agent moving further away from the goal positions.\n",
        "*   Later Stages: After training for many episodes, the agent learns to make optimal decisions. The visualization shows the agent making quick and direct movements towards the item, collecting it, and then moving efficiently to the final goal position (G).\n",
        "\n",
        "<p>plot_rewards(): Visualizes total rewards per episode to show learning progress.</p>\n",
        "<p>plot_epsilon_decay(): Shows how the exploration rate (epsilon) decreases over episodes, indicating the shift from exploration to exploitation.<p>\n",
        "<p>plot_validation_scores(): plots the validation scores</p>\n",
        "<p>Also in plots folder: we get a plot of the networks metrics every 10 episodes and final ending metrics</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During actual training, we also used MLFlow to keep track of loss, average rewards, validation metric score (explained later on), etc. We see that at around episode of 250 we get the highest validation score and then it gradually starts overfitting.\n",
        "![Training Visualization](training_visualization.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "collapsed": true,
        "id": "UXUXzzgaK4QD",
        "outputId": "bfd43596-9702-4f4b-f49b-77ef30141f41"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        agent: DQNAgent, \n",
        "        environment: Assignment2Environment, \n",
        "        with_log: bool = False, \n",
        "        log_step: int = 100, \n",
        "        update_target_episodes: int = 20, \n",
        "        num_validation_episodes: int = 30, \n",
        "        save_checkpoint_interval: int = 50,  # in episodes\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Trainer with the DQN agent and environment.\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.environment = environment\n",
        "        \n",
        "        self.update_target_episodes = update_target_episodes\n",
        "        \n",
        "        self.episode_rewards: list[float] = []\n",
        "        self.validation_scores: list[float] = []\n",
        "        \n",
        "        self.with_log = with_log\n",
        "        self.global_step = 0\n",
        "        self.log_step = log_step\n",
        "        self.num_validation_episodes = num_validation_episodes\n",
        "        \n",
        "        self.save_checkpoint_interval = save_checkpoint_interval\n",
        "\n",
        "        # Initialize the Plotter\n",
        "        self.plotter = Plotter(save_dir=\"./plots\")\n",
        "\n",
        "\n",
        "    def train_one_episode(self, epoch_idx: int) -> None:\n",
        "        \"\"\"\n",
        "        Conducts training for a single episode.\n",
        "        \"\"\"\n",
        "        self.environment.initialize_for_new_episode()\n",
        "\n",
        "        current_state = self.environment.get_state()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        while not done:\n",
        "            state_array = self.state_to_array(current_state)\n",
        "            available_actions = self.environment.get_available_actions(current_state)\n",
        "            action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions)\n",
        "            reward, next_state = self.environment.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "            # print(f\"S_t={current_state}, A={action.name}, R={reward}, S_t+1={next_state}\")\n",
        "            next_state_array = self.state_to_array(next_state)\n",
        "            done = self.environment.is_goal_state(next_state)\n",
        "            total_reward += reward\n",
        "            \n",
        "            self.agent.replay_buffer.remember((state_array, action.value, reward, next_state_array, done))\n",
        "            self.agent.replay()  # maybe train inside\n",
        "            \n",
        "            current_state = next_state\n",
        "            step_count += 1\n",
        "            self.global_step += 1\n",
        "\n",
        "        # decrease exploration over time\n",
        "        self.agent.epsilon = max(self.agent.epsilon_min, self.agent.epsilon * self.agent.epsilon_decay)\n",
        "        self.episode_rewards.append(total_reward)\n",
        "\n",
        "    def state_to_array(self, state: Assignment2State) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a State object into a numpy array suitable for input to the DQN.\n",
        "        \"\"\"\n",
        "        # Convert Assignment2State to array\n",
        "        return np.array([\n",
        "            *state.agent_location,  # Agent's (x, y) location\n",
        "            *state.item_location,   # Item's (x, y) location\n",
        "            float(state.has_item),  # 1 if agent has item, 0 otherwise\n",
        "            *state.goal_location,   # Goal's (x, y) location\n",
        "            *state.goal_direction,  # Direction to goal (dx, dy)\n",
        "            *state.item_direction   # Direction to item (dx, dy)\n",
        "        ])\n",
        "\n",
        "    def train(self, num_episodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Train the agent across multiple episodes.\n",
        "        \"\"\"\n",
        "        num_nn_passes = 0\n",
        "        current_best_validation_score = -float('inf')\n",
        "\n",
        "        for episode in range(1, num_episodes+1):\n",
        "            print(f\"Starting Episode {episode + 1}\")\n",
        "            self.train_one_episode(episode)\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                self.plotter.update_plot(self.agent.logged_data)\n",
        "\n",
        "            if episode % self.update_target_episodes == 0:\n",
        "                self.agent.update_target_network()\n",
        "                if self.with_log:\n",
        "                    print(\"Target network updated\")\n",
        "            print(f\"Episode {episode + 1} completed. Epsilon: {self.agent.epsilon:.4f}\")\n",
        "            if self.agent.steps != num_nn_passes:\n",
        "                validation_score = self.validate(episode)\n",
        "                self.validation_scores.append(validation_score)\n",
        "                if validation_score > current_best_validation_score:\n",
        "                    print(f\"New best validation score: {validation_score}\")\n",
        "                    current_best_validation_score = validation_score\n",
        "                    self.save_agent(episode)\n",
        "                self.visualize_sample_episode()\n",
        "                num_nn_passes = self.agent.steps\n",
        "            if episode % self.save_checkpoint_interval == 0:\n",
        "                self.save_agent(episode)\n",
        "                \n",
        "        \n",
        "        # Plot and save the rewards and epsilon decay after training is complete\n",
        "        self.plot_rewards(save=True, filename='reward_plot.png')\n",
        "        self.plot_epsilon_decay(num_episodes, save=True, filename='epsilon_decay_plot.png')\n",
        "        self.plot_validation_scores(save=True, filename='validation_score_plot.png')  # Plot validation scores\n",
        "        self.plotter.update_plot(self.agent.logged_data)\n",
        "\n",
        "    def visualize_sample_episode(self) -> None:\n",
        "        sample_env = Assignment2Environment(n=4, with_animation=True)\n",
        "        sample_env.initialize_for_new_episode()\n",
        "        current_state = sample_env.get_state()\n",
        "        start_time = time.time()\n",
        "        done = False\n",
        "        \n",
        "        prev_state = None\n",
        "        \n",
        "        while not done and time.time() - start_time < 1*20:\n",
        "            state_array = self.state_to_array(current_state)\n",
        "            available_actions = sample_env.get_available_actions(current_state)\n",
        "            action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions, is_test=True)\n",
        "            reward, next_state = sample_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "            done = sample_env.is_goal_state(next_state)\n",
        "            \n",
        "            # check for three-step cycle and stop early\n",
        "            if next_state == prev_state:\n",
        "                print(\"cycle detected... breaking\")\n",
        "                break\n",
        "            prev_state = current_state\n",
        "            current_state = next_state\n",
        "        \n",
        "        plt.close('all')\n",
        "\n",
        "    def validate(self, current_episode_index: int):\n",
        "        calulated_scores = []\n",
        "        for _ in range(self.num_validation_episodes):\n",
        "            sample_env = Assignment2Environment(n=4, with_animation=False)\n",
        "            sample_env.initialize_for_new_episode()\n",
        "            sample_env.current_sub_environment.agent.has_item = False # metric assumes that agent starts without item\n",
        "            current_state = sample_env.get_state()\n",
        "            start_time = time.time()\n",
        "            done = False\n",
        "            start_location = sample_env.current_sub_environment.agent.get_location()\n",
        "            item_location = sample_env.current_sub_environment.item.get_location()\n",
        "            goal_location = sample_env.current_sub_environment.goal_location\n",
        "            \n",
        "            prev_state = None\n",
        "            predicted_steps = 0\n",
        "            while not done:\n",
        "                if time.time() - start_time > 20:\n",
        "                    predicted_steps = 0\n",
        "                    break\n",
        "                state_array = self.state_to_array(current_state)\n",
        "                available_actions = sample_env.get_available_actions(current_state)\n",
        "                action, is_greedy, all_qvals = self.agent.select_action(state_array, available_actions, is_test=True)\n",
        "                reward, next_state = sample_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "                done = sample_env.is_goal_state(next_state)\n",
        "                \n",
        "                # check for three-step cycle and stop early\n",
        "                if next_state == prev_state:\n",
        "                    predicted_steps = 0\n",
        "                    break\n",
        "                prev_state = current_state\n",
        "                current_state = next_state\n",
        "                predicted_steps += 1\n",
        "            calulated_scores.append(calculate_metrics_score(predicted_steps, start_location, item_location, goal_location))\n",
        "        \n",
        "        result = sum(calulated_scores) / self.num_validation_episodes\n",
        "\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_agent(self, episode_index: int) -> None:\n",
        "        save_path = Path(f\"checkpoints/episode_{episode_index}.pt\")\n",
        "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.agent.save_state(save_path)\n",
        "\n",
        "    def plot_rewards(self, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the total reward earned per episode.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.episode_rewards, label='Total Reward per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "        plt.title('Reward Earned per Episode')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Reward plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def plot_validation_scores(self, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the validation scores over episodes.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.validation_scores, label='Validation Score per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Validation Score')\n",
        "        plt.title('Validation Score Over Episodes')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Validation score plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def plot_epsilon_decay(self, num_episodes: int, save: bool = False, filename: str | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Plot the epsilon decay over episodes.\n",
        "        \"\"\"\n",
        "        epsilons = [max(self.agent.epsilon_min, self.agent.epsilon * (self.agent.epsilon_decay ** i)) for i in range(num_episodes)]\n",
        "        \n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(range(num_episodes), epsilons, label='Epsilon Decay')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.title('Epsilon Decay over Episodes')\n",
        "        plt.legend()\n",
        "        if save and filename:\n",
        "            plt.savefig(filename)\n",
        "            print(f\"Epsilon decay plot saved to {filename}\")\n",
        "        else:\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBiEKEQjkSgC"
      },
      "source": [
        "### Metric:\n",
        "$$ \\frac{1}{number\\ of\\ episodes}\\sum_{item\\ location}\\sum_{agent\\ location}\\sum_{goal\\ location}{\\frac{M(agent\\ location, item\\ location) + 1 + M(item\\ location, goal\\ location)}{number\\ of\\ steps\\ taken}} $$\n",
        "where M represents Manhattan distance\n",
        "\n",
        "  We began by training our DQN for all possible item locations and goal locations. After training, we evaluated the agent's performance by calculating the ratio of the shortest possible distance (using Manhattan distance) to the actual number of steps taken by the agent from the start position, through the item, and finally reaching the goal (shortest distance / actual distance taken by the agent). This calculation was done for all possible agent, item and goal locations within the environment. A ratio of 1 indicates that the actual path matches the shortest path, while a ratio less than 1 suggests a longer path was taken. By averaging these ratios across all possible scenarios, we can obtain the metric.\n",
        "  \n",
        "  We obtained an average metric value approx 0.9352, indicating that our DQN effectively learns the optimal paths in the environment. Note that 1 in the metric represents additional step to pick up item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PopRHtiPD2sV"
      },
      "outputs": [],
      "source": [
        "def calculate_manhattan_distance(start_location: tuple[int, int], goal_location: tuple[int, int]) -> int:\n",
        "    \"\"\"\n",
        "    Calculates the Manhattan distance between two points.\n",
        "    \"\"\"\n",
        "    start_x, start_y = start_location\n",
        "    goal_x, goal_y = goal_location\n",
        "    return abs(start_x - goal_x) + abs(start_y - goal_y)\n",
        "\n",
        "def calculate_metrics_score(predicted_distance: int, start_location: tuple[int, int], item_location: tuple[int, int], goal_location: tuple[int, int]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the proportion of the distance to the shortest distance.\n",
        "    \"\"\"\n",
        "    # Calculate shortest distance from start to item to goal\n",
        "    shortest_distance = (\n",
        "        calculate_manhattan_distance(start_location, item_location)\n",
        "        + 1\n",
        "        + calculate_manhattan_distance(item_location, goal_location)\n",
        "    )\n",
        "    return (shortest_distance / predicted_distance) if predicted_distance != 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kqwlt3kB7-IR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:40<00:00,  6.25s/it]\n",
            "0it [01:40, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average performance score (1 is the best): 0.9352\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, n=4) -> None:\n",
        "        self.n = n\n",
        "        self.dqn_envs = Assignment2Environment(n=4, with_animation=False)\n",
        "        self.dqn_agent = DQNAgent(with_log=True)\n",
        "\n",
        "    def run_dqn_train(self):\n",
        "        \"\"\"\n",
        "        Trains DQN agent in the environment and save the states.\n",
        "        \"\"\"\n",
        "        trainer = Trainer(self.dqn_agent, self.dqn_envs, with_log=True)\n",
        "        trainer.train(num_episodes=300)\n",
        "        self.dqn_agent.save_state(\"trained_dqn.pth\")\n",
        "\n",
        "    def load_trained_dqn(self, path: str):\n",
        "        \"\"\"\n",
        "        Load the saved DQN\n",
        "        \"\"\"\n",
        "        self.dqn_agent.load_state(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_grid_location_list(max_x: int, max_y) -> list[tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Generate the grid location list for all possible cases\n",
        "        \"\"\"\n",
        "        return [(i, j) for i in range(max_x) for j in range(max_y)]\n",
        "\n",
        "    def state_to_array(self, state: State) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a State object into a numpy array suitable for input to the DQN.\n",
        "        \"\"\"\n",
        "        # Check if the state is an instance of Assignment2State\n",
        "        if isinstance(state, Assignment2State):\n",
        "            # Convert Assignment2State to array\n",
        "            state_array = np.array([\n",
        "                *state.agent_location,  # Agent's (x, y) location\n",
        "                *state.item_location,   # Item's (x, y) location\n",
        "                float(state.has_item),  # 1 if agent has item, 0 otherwise\n",
        "                *state.goal_location,   # Goal's (x, y) location\n",
        "                *state.goal_direction,  # Direction to goal (dx, dy)\n",
        "                *state.item_direction   # Direction to item (dx, dy)\n",
        "            ])\n",
        "        else:\n",
        "            # Convert basic State to array (without goal-related information)\n",
        "            state_array = np.array([\n",
        "                *state.agent_location,  # Agent's (x, y) location\n",
        "                *state.item_location,   # Item's (x, y) location\n",
        "                float(state.has_item)   # 1 if agent has item, 0 otherwise\n",
        "            ])\n",
        "\n",
        "        # Ensure the state array matches the input size of the neural network\n",
        "        if len(state_array) != 11:\n",
        "            print(f\"Warning: State array length mismatch. Expected 11, got {len(state_array)}. Padding with zeros.\")\n",
        "            state_array = np.pad(state_array, (0, 11 - len(state_array)), 'constant')\n",
        "        return state_array\n",
        "\n",
        "    def dqn_performance_test(self):\n",
        "        \"\"\"\n",
        "        Conducts a performance test for DQN. The maximum of the score is 1.\n",
        "        \"\"\"\n",
        "        num_episodes = 0\n",
        "        total_score = 0\n",
        "\n",
        "        # Loop over all environments in DQN environment\n",
        "        for i, _ in tqdm(enumerate(self.dqn_envs.environments)):\n",
        "            for agent_location in tqdm(self.generate_grid_location_list(self.n, self.n)):\n",
        "                # Initialize episode with a given agent location\n",
        "                self.dqn_envs.initialize_for_new_episode(agent_location=agent_location, env_index=i)\n",
        "\n",
        "                # Ensure agent location is not same place with item and goal\n",
        "                if agent_location == self.dqn_envs.current_sub_environment.item.get_location() or agent_location == self.dqn_envs.current_sub_environment.goal_location:\n",
        "                    continue\n",
        "\n",
        "                # Metric assumes that agent starts without item\n",
        "                self.dqn_envs.current_sub_environment.agent.has_item = False\n",
        "\n",
        "                # Get start, item, and goal location to calcurate distance\n",
        "                start_location = self.dqn_envs.current_sub_environment.agent.get_location()\n",
        "                item_location = self.dqn_envs.current_sub_environment.item.get_location()\n",
        "                goal_location = self.dqn_envs.current_sub_environment.goal_location\n",
        "\n",
        "                current_state = self.dqn_envs.get_state() # get current state\n",
        "                start_time = time.time() # to keeps track time\n",
        "                predicted_steps = 0 # count the number of actual steps taken\n",
        "                done = False # for one environment\n",
        "                is_break = False # to keep track the break\n",
        "\n",
        "                while not done:\n",
        "                    # Break if it takes more than 20 seconds\n",
        "                    if time.time() - start_time > 20:\n",
        "                        is_break = True\n",
        "                        break\n",
        "                    state_array = self.state_to_array(current_state) # get the states in array format\n",
        "                    available_actions = self.dqn_envs.get_available_actions(current_state) # get available actions\n",
        "                    action, is_greedy, all_qvals = self.dqn_agent.select_action(state_array, available_actions, is_test=True)\n",
        "                    reward, next_state = self.dqn_envs.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals) # get next state\n",
        "                    done = self.dqn_envs.is_goal_state(next_state) # Check if it is goal position\n",
        "                    current_state = next_state # update current state\n",
        "                    predicted_steps += 1\n",
        "\n",
        "                if not is_break:\n",
        "                    # calculate the metrics score\n",
        "                    total_score += calculate_metrics_score(predicted_steps, start_location, item_location, goal_location)\n",
        "                    num_episodes += 1 # increase the episode\n",
        "\n",
        "            # Return the average score across all possible tests\n",
        "            return (total_score / num_episodes) if num_episodes != 0 else 0\n",
        "\n",
        "    def visualize_dqn(self, num_of_vis: int = 5) -> None:\n",
        "        \"\"\"\n",
        "        Visualize the path after trained for given times\n",
        "        \"\"\"\n",
        "        for _ in (0, num_of_vis):\n",
        "            self.dqn_envs.set_with_animation(True) # Set the animation True\n",
        "            self.dqn_envs.initialize_for_new_episode()\n",
        "            self.dqn_envs.current_sub_environment.agent.has_item = False # Assumes that agent starts without item\n",
        "\n",
        "            current_state = self.dqn_envs.get_state()\n",
        "            start_time = time.time() # Keep track time\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # If it takes more than 20 seconds to reach the goal, break the loop\n",
        "                if time.time() - start_time > 20:\n",
        "                    break\n",
        "                state_array = self.state_to_array(current_state) # get the states in array format\n",
        "                available_actions = self.dqn_envs.get_available_actions(current_state) # get available actions\n",
        "                action, is_greedy, all_qvals = self.dqn_agent.select_action(state_array, available_actions, is_test=True)\n",
        "                reward, next_state = self.dqn_envs.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals) # get next state\n",
        "                done = self.dqn_envs.is_goal_state(next_state) # Check if it is goal position\n",
        "                current_state = next_state # update current state\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # DQN\n",
        "    evl = Evaluation()\n",
        "\n",
        "    # Training DQN\n",
        "    # evl.run_dqn_train()  # UNCOMMENT TO TRAIN DQN\n",
        "\n",
        "    saved_path = Path(\"episode_250.pt\")\n",
        "    evl.load_trained_dqn(saved_path)\n",
        "\n",
        "    # Conduct the performance test\n",
        "    average_score = evl.dqn_performance_test()\n",
        "    print(f\"Average performance score (1 is the best): {average_score:.4f}\")\n",
        "\n",
        "    # Visualize randomly the environments and show the steps of the agent\n",
        "    evl.visualize_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBzklVnunW2_"
      },
      "source": [
        "**<p>Hyperparameter Tuning</p>**\n",
        "\n",
        "<p> We performed hyperparameter tuning for the DQN agent using library Optuna. An objective function defined to evaluate different sets of hyperparameters and optimize them to maximize the DQN agent's total reward.</p>\n",
        "\n",
        "<p>We set a maximum amount of time allowed of 10 seconds for each trial while running optimisation.</p>\n",
        "\n",
        "<p>Below are the sets of hyperparameters used in tuning:</p>\n",
        "\n",
        "<ol>\n",
        "<li>Optimizer learning rate <i>(alpha)</i> from range of 0.995 to 0.999</li>\n",
        "<li>Discount factor for future rewards <i>(discount_rate)</i> from range of 0.95 to 0.975</li>\n",
        "<li>Exploration rate <i>(epsilon)</i> with a set of 0.2, 0.5, 0.4 and 0.6</li>\n",
        "<li>Maximum size of replay memory <i>(replay_memory_size)</i> from range of 1000 to 5000 </li>\n",
        "<li>Batch size for experience replay <i>(batch_size)</i> with a set of 16, 32, 64, 128, 256 </li>\n",
        "<li>Time penalty <i>(time_penalty)</i>  from range of -10 to -1</li>\n",
        "<li>Penalty for reach goal without item <i>(goal_no_item_penalty)</i> from range of -500 to -100</li>\n",
        "<li>Penalty for revisiting the item location <i>(item_revisit_penalty)</i> from range of -200 to -50</li>\n",
        "<li>Reward for collecting the item <i>(item_state_reward)</i> from range of 100 to 200</li>\n",
        "<li>Reward for reaching the goal with item collected <i>(goal_state_reward)</i> from range of 300 to 600</li>\n",
        "<li>Number of epsiodes to train the model <i>(num_episodes)</i> from range of 100 to 600</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "<p>We then initialised the environment and agent with the sets of hyperparameters and start training loop.</p>\n",
        "\n",
        "<p>We call Optuna to start the optimsation process with 50 number of trials. We run this code as script but did not have time to repeat it on this notebook. The default values of the hyper-parameters are extracted from the result of the hyper-parameter tuning.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-18 01:44:01,534] A new study created in memory with name: no-name-bf5cf399-1de5-4fee-ae22-6b1a4a77201b\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d997bf2ab974c31b04c153b19727389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[W 2024-09-18 01:44:15,869] Trial 0 failed with parameters: {'alpha': 0.9987391184032246, 'discount_rate': 0.9545600361942512, 'epsilon': 0.45, 'replay_memory_size': 2932, 'batch_size': 64, 'time_penalty': -7, 'goal_no_item_penalty': -396, 'item_revisit_penalty': -102, 'item_state_reward': 128, 'goal_state_reward': 334, 'num_episodes': 184} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_29896\\3834780192.py\", line 88, in objective\n",
            "    tune_agent.replay()\n",
            "  File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_29896\\3274963641.py\", line 192, in replay\n",
            "    max_future_q = self.get_double_q(next_states[i])\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_29896\\3274963641.py\", line 133, in get_double_q\n",
            "    best_action_index = torch.argmax(self.model(state_tensor)[0]).item()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2024-09-18 01:44:15,881] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m tuning \u001b[38;5;241m=\u001b[39m Tuning()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Conduct hyperparameter tuning\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m tuning\u001b[38;5;241m.\u001b[39mrun_hyperparameter_tuning()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Visualize the hyperparameter tuning\u001b[39;00m\n\u001b[0;32m    136\u001b[0m tuning\u001b[38;5;241m.\u001b[39mhyperparameter_tuning_visualization()\n",
            "Cell \u001b[1;32mIn[11], line 113\u001b[0m, in \u001b[0;36mTuning.run_hyperparameter_tuning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m num_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Optimize the objective function\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective, n_trials\u001b[38;5;241m=\u001b[39mnum_trials, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters and the best value\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params)\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[11], line 88\u001b[0m, in \u001b[0;36mTuning.objective\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     85\u001b[0m tune_agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mremember((state_array, action\u001b[38;5;241m.\u001b[39mvalue, reward, next_state_array, tune_env\u001b[38;5;241m.\u001b[39mis_goal_state(next_state)))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Learn from experiences using experience replay\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m tune_agent\u001b[38;5;241m.\u001b[39mreplay()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[0;32m     91\u001b[0m current_state \u001b[38;5;241m=\u001b[39m next_state\n",
            "Cell \u001b[1;32mIn[7], line 192\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m         targets\u001b[38;5;241m.\u001b[39mappend(rewards[i])\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# max_future_q = self.get_maxQ(next_states[i])\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m         max_future_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_double_q(next_states[i])\n\u001b[0;32m    193\u001b[0m         targets\u001b[38;5;241m.\u001b[39mappend(rewards[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_rate \u001b[38;5;241m*\u001b[39m max_future_q)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "Cell \u001b[1;32mIn[7], line 133\u001b[0m, in \u001b[0;36mDQNAgent.get_double_q\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    131\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 133\u001b[0m     best_action_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state_tensor)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    134\u001b[0m     max_qval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model(state_tensor)[\u001b[38;5;241m0\u001b[39m][best_action_index]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m max_qval\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import optuna\n",
        "import time\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "TIME_LIMIT = 10\n",
        "\n",
        "\n",
        "class Tuning:\n",
        "    def __init__(self, time_limit: int = TIME_LIMIT) -> None:\n",
        "        self.study = optuna.create_study(direction='maximize') # Create an Optuna study\n",
        "        self.time_limit = time_limit\n",
        "\n",
        "    def objective(self, trial: optuna.Trial) -> float:\n",
        "        # Hyperparameters to optimize\n",
        "        alpha = trial.suggest_float('alpha', 0.995, 0.999)\n",
        "        discount_rate = trial.suggest_float('discount_rate', 0.95, 0.975)\n",
        "        epsilon = trial.suggest_categorical('epsilon', [0.2, 0.35, 0.45, 0.6])\n",
        "        replay_memory_size = trial.suggest_int('replay_memory_size', 1000, 5000)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
        "        time_penalty = trial.suggest_int('time_penalty', -10, -1)\n",
        "        goal_no_item_penalty = trial.suggest_int('goal_no_item_penalty', -500, -100)\n",
        "        item_revisit_penalty = trial.suggest_int('item_revisit_penalty', -200, -50)\n",
        "        item_state_reward = trial.suggest_int('item_state_reward', 100, 200)\n",
        "        goal_state_reward = trial.suggest_int('goal_state_reward', 300, 600)\n",
        "        num_episodes = trial.suggest_int('num_episodes', 100, 600)\n",
        "\n",
        "        # Initialize Assignment2Environment\n",
        "        tune_env = Assignment2Environment(\n",
        "        n=4,  # Grid size\n",
        "        time_penalty=time_penalty,\n",
        "        goal_no_item_penalty=goal_no_item_penalty,\n",
        "        item_revisit_penalty=item_revisit_penalty,\n",
        "        item_state_reward=item_state_reward,\n",
        "        goal_state_reward=goal_state_reward,\n",
        "        direction_reward_multiplier=1,\n",
        "        with_animation=False\n",
        "        )\n",
        "\n",
        "        # Initialize DQNAgent\n",
        "        tune_agent = DQNAgent(\n",
        "        alpha=alpha,\n",
        "        discount_rate=discount_rate,\n",
        "        epsilon=epsilon,\n",
        "        replay_memory_size=replay_memory_size,\n",
        "        batch_size=batch_size,\n",
        "        )\n",
        "\n",
        "        # num_episodes = 200 # define episodes\n",
        "        tune_trainer = Trainer(tune_agent, tune_env)\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            tune_env.initialize_for_new_episode() # Initialize the environment for a new episode\n",
        "            current_state = tune_env.get_state()  # Get state from current sub-environment\n",
        "            total_reward = 0  # Track total reward for the episode\n",
        "            start_time = time.time()  # Record the start time of the episode\n",
        "\n",
        "            while not tune_env.is_goal_state(current_state):\n",
        "                # Check if time limit is exceeded\n",
        "                elapsed_time = time.time() - start_time\n",
        "                if elapsed_time > self.time_limit:\n",
        "                    break\n",
        "\n",
        "                # Convert the current state to a numpy array for input to the neural network\n",
        "                state_array = tune_trainer.state_to_array(current_state)\n",
        "\n",
        "                # Retrieve available actions from the current sub-environment\n",
        "                available_actions = tune_env.get_available_actions(current_state)\n",
        "\n",
        "                # Select an action using the agent's Îµ-greedy policy\n",
        "                action, is_greedy, all_qvals = tune_agent.select_action(state_array, available_actions)\n",
        "\n",
        "                # Execute the action in the current sub-environment, receive reward and next state\n",
        "                reward, next_state = tune_env.step(action=action, is_greedy=is_greedy, all_qvals=all_qvals)\n",
        "\n",
        "\n",
        "                # Add the reward to the total reward for this episode\n",
        "                total_reward += reward\n",
        "\n",
        "                # Convert the next state to a numpy array\n",
        "                next_state_array = tune_trainer.state_to_array(next_state)\n",
        "\n",
        "                # Store experience in the agent's replay memory\n",
        "                tune_agent.replay_buffer.remember((state_array, action.value, reward, next_state_array, tune_env.is_goal_state(next_state)))\n",
        "\n",
        "                # Learn from experiences using experience replay\n",
        "                tune_agent.replay()\n",
        "\n",
        "                # Move to the next state\n",
        "                current_state = next_state\n",
        "\n",
        "            # decrease exploration over time\n",
        "            tune_agent.epsilon = max(tune_agent.epsilon_min, tune_agent.epsilon * tune_agent.epsilon_decay)\n",
        "            # Store total reward of the episode\n",
        "            tune_trainer.episode_rewards.append(total_reward)\n",
        "\n",
        "            # Prune the trial early if it's performing poorly\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "\n",
        "    def run_hyperparameter_tuning(self):\n",
        "        '''\n",
        "        Run hyperparameter tunig and save the best parameters\n",
        "        '''\n",
        "        # Define the number of trials\n",
        "        num_trials = 50\n",
        "\n",
        "        # Optimize the objective function\n",
        "        self.study.optimize(self.objective, n_trials=num_trials, show_progress_bar=True)\n",
        "\n",
        "        # Print the best hyperparameters and the best value\n",
        "        print(\"Best Hyperparameters: \", self.study.best_params)\n",
        "        print(\"Best Value: \", self.study.best_value)\n",
        "\n",
        "        # Save the best hyperparameters to a YAML file\n",
        "        with open(\"config.yml\", \"w\") as file:\n",
        "            yaml.dump(self.study.best_params, file, default_flow_style=False)\n",
        "\n",
        "    def hyperparameter_tuning_visualization(self):\n",
        "        '''\n",
        "        Visualize the optimization history\n",
        "        '''\n",
        "        optuna.visualization.plot_optimization_history(self.study)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tuning = Tuning()\n",
        "\n",
        "    # Conduct hyperparameter tuning\n",
        "    tuning.run_hyperparameter_tuning()\n",
        "\n",
        "    # Visualize the hyperparameter tuning\n",
        "    tuning.hyperparameter_tuning_visualization()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
